{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9cd8d85-eb4a-45d6-925a-e3e770592043",
   "metadata": {},
   "source": [
    "# Leven at Newby Bridge using HBV model in eWaterCycle\n",
    "This notebook demonstrates how eWaterCycle is used to run a model, in this case the classic HBV model, with forcing from the Caravan dataset. The only difference with the previous notebook is that here we use a \"local\" version of HBV, so HBV is not run inside of a container. This greatly speeds up the performance, but is less 'Reproducible' because now the results depend (more) on the setting / python version / dependcies of the local machine.\n",
    "\n",
    "For this example, we chose a catchment in the Lake District in the UK. River discharge in river Leven is measured at the weirs at Newby Bridge, see the photo below. This observational data is available through the CamelsGB dataset [(Coxon, 2020)](https://essd.copernicus.org/articles/12/2459/2020/).\n",
    "The larger [Caravan](https://www.nature.com/articles/s41597-023-01975-w) (collection of Camels...) dataset is available through eWaterCycle and will be used below.\n",
    "\n",
    "![image](https://upload.wikimedia.org/wikipedia/commons/7/76/Weirs_on_the_River_Leven_at_Newby_Bridge_-_geograph.org.uk_-_5455774.jpg)\n",
    "\n",
    "*Weirs on the River Leven at Newby Bridge by G Laird*\n",
    "\n",
    "As a model we choose the classic [HBV model](https://hess.copernicus.org/articles/26/1371/2022/) for its simplicity. This nicely demonstrates how eWaterCycle works. More complex models are available through eWaterCycle, however:\n",
    "\n",
    "- these often require parameter sets specific to a region\n",
    "- these are more computationally intensive to run and therefore unsuited for a short 45 minute workshop.\n",
    "\n",
    "Ask us about available models if you want to collaborate! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641fbd17",
   "metadata": {},
   "source": [
    "### HBV model structure\n",
    "\n",
    "The stucture of the HBV model is shown below.\n",
    "Do note that the figure is missing a snow reservoir, however this has been implemented in the HBV model in this notebook.\n",
    "\n",
    "![image](_images/model_layout.png)\n",
    "\n",
    "*Image from the TU Delft course ENVM1502 - \"River Basin Hydrology\" by Markus Hrachowitz* \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73ddde7-bde3-42e8-991a-8edd4825fd78",
   "metadata": {},
   "source": [
    "## Starting up\n",
    "To start up, we need to import eWaterCycle and a number of general Python libraries. 'Under the hood' eWaterCycle depends on a large number of other pieces of software, including but not limited to\n",
    "\n",
    "- [grpc4bmi](https://github.com/eWaterCycle/grpc4bmi), a 'translator' for BMI function calls between different programming languages and across containers. This library was build by the eWaterCycle team, but is available openly for anyone that can benefit from its functionality. \n",
    "- apptainer, a container engine that runs the model-containers (Docker is supported too).\n",
    "- [ESMValTool](https://github.com/ESMValGroup/ESMValTool), a climate data processing toolbox that originally intended to post-process climate data from CMIP projects for inclusion in IPCC reports, we adopted as tool for pre-processing climate data into forcing data for hydrological models\n",
    "- Numerous hydrological models that are made available as plugins to eWaterCycle, see [eWaterCycle-leakybucket](https://github.com/eWaterCycle/ewatercycle-leakybucket) as an example. Note that plugins do not have to be owned and maintained by the eWaterCycle team: anyone with a model can make a plugin for eWaterCycle and make their model be available for others through the platform. \n",
    "\n",
    "Furthermore, eWaterCycle requires forcing data, obsrvational data and parameter sets to be available to users. If you want to install eWaterCycle on your own infrastructure, see the [eWaterCycle documentation](https://ewatercycle.readthedocs.io/en/latest/system_setup.html) or contact us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec7e710a-5aa4-40f9-a1cb-151e3cddbe04",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-07T14:21:32.492461800Z",
     "start_time": "2024-03-07T14:21:31.890843300Z"
    }
   },
   "outputs": [],
   "source": [
    "# general python\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "\n",
    "#niceties\n",
    "from rich import print\n",
    "\n",
    "#needed\n",
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4569a0f2-4bea-48cc-b5a4-ca5384e368c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-07T14:21:34.475126400Z",
     "start_time": "2024-03-07T14:21:32.537461400Z"
    }
   },
   "outputs": [],
   "source": [
    "# general eWaterCycle\n",
    "import ewatercycle\n",
    "import ewatercycle.models\n",
    "import ewatercycle.forcing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ecd862-3e33-4001-9218-d025f6acc2ae",
   "metadata": {},
   "source": [
    "## Choose region and time period: \n",
    "The HBV model is a lumped hydrological model. It considers a catchment as a homogenious area and calculates the major hydrological processes in it. It requires spatially aggregated rainfall and potential evaporation as inputs (ie. forcings). To calculate its modelled discharge at the outlet of the catchment it also needs a set of parameters. Usually these paramters are calibrated using (historical) observational data, so this is also required. \n",
    "\n",
    "in eWaterCycle we provide access to the Caravan dataset, which contains all of the above data for all the catchments in the different Camels datasets. In this notebook we use the precipitation and evaporation data from Caravan. However, there is a known problem with the caravan evaporation data and (the current version) shouldn't be used. In this notebook we do so to demonstrate how it works. It would be better to generate forcing from (for example) ERA5. [This](example_model_run_HBV_camels_catchment_ERA5_forcing.ipynb) additional notebook that looks very similar to this one explains how to do that. \n",
    "\n",
    "Using the interactive maps at [eWaterCycle caravan map](https://www.ewatercycle.org/caravan-map/) one can easily retrieve the identifier of the catchment.\n",
    "\n",
    "Note that changing the region below will work, but that the parameters that are loaded later in this notebook are calibrated specifically for this particular catchment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5898939-54a6-40de-8a27-bbc793536248",
   "metadata": {},
   "outputs": [],
   "source": [
    "camelsgb_id = \"lamah_208082\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b5fc86-ec75-46a8-9f40-4685444628e5",
   "metadata": {},
   "source": [
    "We have to specify start and end date of the experiment that we want to do. For now we don't fuzz with diverences between calibration and validation periods (which officially of course is very bad...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03aea008-87ce-4d09-8d01-f12dfe6bb116",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-07T14:21:34.567126600Z",
     "start_time": "2024-03-07T14:21:34.567126600Z"
    }
   },
   "outputs": [],
   "source": [
    "experiment_start_date = \"1981-08-01T00:00:00Z\"\n",
    "experiment_end_date = \"2010-08-31T00:00:00Z\"\n",
    "\n",
    "calibration_start_time = experiment_start_date\n",
    "calibration_end_time = experiment_end_date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111bc65b-8299-43ba-95fd-e92df6b92707",
   "metadata": {},
   "source": [
    "## Set up paths\n",
    "\n",
    "Since forcing files are often re-used between experiments it is a best practice to save those intermediate files for re-use between experiments. These logical save-points in workflows are called 'rustpunten' in Dutch. It is important to store data in 'rustpunten' in standard formats. Working with clearly defined 'rustpunten' is a key element in the design of good workflows in general and was instrumental in designing eWaterCycle in particular. \n",
    "\n",
    "Here we set up some paths to store the forcing files we generate in your own home directory. \n",
    "\n",
    "To speed up this workshop, we have already created the forcing files in a central location, which we also create pointers to here. If you want to run for a different region, you will have to generate the forcing yourself. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb5ed837-14aa-4629-b589-ffc33c24ba84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Even though we don't use caravan data as forcing, we still call it forcing\n",
    "# because it is generated using the forcing module of eWaterCycle\n",
    "forcing_path_caravan = Path.home() / \"forcing\" / camelsgb_id / \"caravan\"\n",
    "forcing_path_caravan.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# If someone has prepared forcing for you, this path needs to be changed to that location. \n",
    "#prepared_forcing_path_caravan_central = Path(\"~/forcing/camelsgb_73010/caravan\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1171ca7e-d26a-4cbe-abb0-d741315a708c",
   "metadata": {},
   "source": [
    "## Generate or load forcing\n",
    "There are three options for creating forcing data objects:\n",
    "\n",
    "- generate from climate data such as Caravan, ERA5 or CMIP. Note that if the directory you specify as destination already contains data this trying this will throw an error!\n",
    "- load forcing data you generated previously by providing the location where it was stored\n",
    "- load forcing data someone else (such as your teacher or a workshop leader) generated previously by providing the location where it was stored\n",
    "\n",
    "First we will create a caravan forcing object, but as mentioned above, we will only use this for the discharge observations and the shape file of the region. After generating the object we show the fields it contains and plot the discharge data. \n",
    "\n",
    "The actual forcing object we will use in this example is the ERA5 based data.\n",
    "\n",
    "For both caravan and ERA5 based forcing, only one of the options provided below should be used, use comments to select which one you want to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19641ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "syntax error, unexpected WORD_WORD, expecting SCAN_ATTR or SCAN_DATASET or SCAN_ERROR\n",
      "context: <!DOCTYPE^ html PUBLIC \"-//W3C//DTD XHTML 1.1//EN\" \"http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd\"><html xmlns=\"http://www.w3.org/1999/xhtml\" xml:lang=\"en\"> <head> <title>The page is temporarily unavailable</title> <meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\" /> <style type=\"text/css\"> /*<![CDATA[*/ body { background-color: #fff; color: #000; font-size: 0.9em; font-family: sans-serif,helvetica; margin: 0; padding: 0; } :link { color: #c00; } :visited { color: #c00; } a:hover { color: #f50; } h1 { text-align: center; margin: 0; padding: 0.6em 2em 0.4em; background-color: #900; color: #fff; font-weight: normal; font-size: 1.75em; border-bottom: 2px solid #000; } h1 strong { font-weight: bold; font-size: 1.5em; } h2 { text-align: center; background-color: #900; font-size: 1.1em; font-weight: bold; color: #fff; margin: 0; padding: 0.5em; border-bottom: 2px solid #000; } h3 { text-align: center; background-color: #ff0000; padding: 0.5em; color: #fff; } hr { display: none; } .content { padding: 1em 5em; } .alert { border: 2px solid #000; } img { border: 2px solid #fff; padding: 2px; margin: 2px; } a:hover img { border: 2px solid #294172; } .logos { margin: 1em; text-align: center; } /*]]>*/ </style> </head> <body> <h1><strong>nginx error!</strong></h1> <div class=\"content\"> <h3>The page you are looking for is temporarily unavailable. Please try again later.</h3> <div class=\"alert\"> <h2>Website Administrator</h2> <div class=\"content\"> <p>Something has triggered missing webpage on your website. This is the default error page for <strong>nginx</strong> that is distributed with Red Hat Enterprise Linux. It is located <tt>/usr/share/nginx/html/50x.html</tt></p> <p>You should customize this error page for your own site or edit the <tt>error_page</tt> directive in the <strong>nginx</strong> configuration file <tt>/etc/nginx/nginx.conf</tt>.</p> <p>For information on Red Hat Enterprise Linux, please visit the <a href=\"http://www.redhat.com/\">Red Hat, Inc. website</a>. The documentation for Red Hat Enterprise Linux is <a href=\"http://www.redhat.com/docs/manuals/enterprise/\">available on the Red Hat, Inc. website</a>.</p> </div> </div> <div class=\"logos\"> <a href=\"http://nginx.net/\"><img src=\"nginx-logo.png\"  alt=\"[ Powered by nginx ]\" width=\"121\" height=\"32\" /></a> <a href=\"http://www.redhat.com/\"><img src=\"poweredby.png\" alt=\"[ Powered by Red Hat Enterprise Linux ]\" width=\"88\" height=\"31\" /></a> </div> </div> </body></html>\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno -70] NetCDF: DAP server error: 'https://opendap.4tu.nl/thredds/dodsC/data2/djht/ca13056c-c347-4a27-b320-930c2a4dd207/1/lamah.nc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/envs/ewatercycle2/lib/python3.12/site-packages/xarray/backends/file_manager.py:211\u001b[0m, in \u001b[0;36mCachingFileManager._acquire_with_cache_info\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 211\u001b[0m     file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_key\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/envs/ewatercycle2/lib/python3.12/site-packages/xarray/backends/lru_cache.py:56\u001b[0m, in \u001b[0;36mLRUCache.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m---> 56\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache\u001b[38;5;241m.\u001b[39mmove_to_end(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: [<class 'netCDF4._netCDF4.Dataset'>, ('https://opendap.4tu.nl/thredds/dodsC/data2/djht/ca13056c-c347-4a27-b320-930c2a4dd207/1/lamah.nc',), 'r', (('clobber', True), ('diskless', False), ('format', 'NETCDF4'), ('persist', False)), '526ecf39-9b8b-4e45-9644-e22ac327f2c3']",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# option one: generate forcing data\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m camelsgb_forcing \u001b[38;5;241m=\u001b[39m \u001b[43mewatercycle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforcing\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msources\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCaravanForcing\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstart_time\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexperiment_start_date\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mend_time\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexperiment_end_date\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdirectory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforcing_path_caravan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbasin_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcamelsgb_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/ewatercycle2/lib/python3.12/site-packages/ewatercycle/_forcings/caravan.py:187\u001b[0m, in \u001b[0;36mCaravanForcing.generate\u001b[0;34m(cls, start_time, end_time, directory, variables, shape, **kwargs)\u001b[0m\n\u001b[1;32m    184\u001b[0m basin_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbasin_id\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    186\u001b[0m dataset: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m basin_id\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 187\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    188\u001b[0m ds_basin \u001b[38;5;241m=\u001b[39m ds\u001b[38;5;241m.\u001b[39msel(basin_id\u001b[38;5;241m=\u001b[39mbasin_id\u001b[38;5;241m.\u001b[39mencode())\n\u001b[1;32m    189\u001b[0m ds_basin_time \u001b[38;5;241m=\u001b[39m crop_ds(ds_basin, start_time, end_time)\n",
      "File \u001b[0;32m/opt/conda/envs/ewatercycle2/lib/python3.12/site-packages/ewatercycle/_forcings/caravan.py:121\u001b[0m, in \u001b[0;36mCaravanForcing.get_dataset\u001b[0;34m(cls, dataset)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_dataset\u001b[39m(\u001b[38;5;28mcls\u001b[39m: \u001b[38;5;28mtype\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCaravanForcing\u001b[39m\u001b[38;5;124m\"\u001b[39m], dataset: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m xr\u001b[38;5;241m.\u001b[39mDataset:\n\u001b[1;32m    109\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Opens specified dataset from data.4tu.nl OPeNDAP server.\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \n\u001b[1;32m    111\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;124;03m            'lamah'\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mxr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mOPENDAP_URL\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdataset\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.nc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/ewatercycle2/lib/python3.12/site-packages/xarray/backends/api.py:670\u001b[0m, in \u001b[0;36mopen_dataset\u001b[0;34m(filename_or_obj, engine, chunks, cache, decode_cf, mask_and_scale, decode_times, decode_timedelta, use_cftime, concat_characters, decode_coords, drop_variables, inline_array, chunked_array_type, from_array_kwargs, backend_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    658\u001b[0m decoders \u001b[38;5;241m=\u001b[39m _resolve_decoders_kwargs(\n\u001b[1;32m    659\u001b[0m     decode_cf,\n\u001b[1;32m    660\u001b[0m     open_backend_dataset_parameters\u001b[38;5;241m=\u001b[39mbackend\u001b[38;5;241m.\u001b[39mopen_dataset_parameters,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    666\u001b[0m     decode_coords\u001b[38;5;241m=\u001b[39mdecode_coords,\n\u001b[1;32m    667\u001b[0m )\n\u001b[1;32m    669\u001b[0m overwrite_encoded_chunks \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite_encoded_chunks\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 670\u001b[0m backend_ds \u001b[38;5;241m=\u001b[39m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdrop_variables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_variables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdecoders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    676\u001b[0m ds \u001b[38;5;241m=\u001b[39m _dataset_from_backend_dataset(\n\u001b[1;32m    677\u001b[0m     backend_ds,\n\u001b[1;32m    678\u001b[0m     filename_or_obj,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    688\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    689\u001b[0m )\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ds\n",
      "File \u001b[0;32m/opt/conda/envs/ewatercycle2/lib/python3.12/site-packages/xarray/backends/netCDF4_.py:666\u001b[0m, in \u001b[0;36mNetCDF4BackendEntrypoint.open_dataset\u001b[0;34m(self, filename_or_obj, mask_and_scale, decode_times, concat_characters, decode_coords, drop_variables, use_cftime, decode_timedelta, group, mode, format, clobber, diskless, persist, auto_complex, lock, autoclose)\u001b[0m\n\u001b[1;32m    644\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mopen_dataset\u001b[39m(\n\u001b[1;32m    645\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    646\u001b[0m     filename_or_obj: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m os\u001b[38;5;241m.\u001b[39mPathLike[Any] \u001b[38;5;241m|\u001b[39m ReadBuffer \u001b[38;5;241m|\u001b[39m AbstractDataStore,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    663\u001b[0m     autoclose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    664\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dataset:\n\u001b[1;32m    665\u001b[0m     filename_or_obj \u001b[38;5;241m=\u001b[39m _normalize_path(filename_or_obj)\n\u001b[0;32m--> 666\u001b[0m     store \u001b[38;5;241m=\u001b[39m \u001b[43mNetCDF4DataStore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    667\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclobber\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclobber\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdiskless\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdiskless\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpersist\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpersist\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mauto_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauto_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautoclose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautoclose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    679\u001b[0m     store_entrypoint \u001b[38;5;241m=\u001b[39m StoreBackendEntrypoint()\n\u001b[1;32m    680\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m close_on_error(store):\n",
      "File \u001b[0;32m/opt/conda/envs/ewatercycle2/lib/python3.12/site-packages/xarray/backends/netCDF4_.py:452\u001b[0m, in \u001b[0;36mNetCDF4DataStore.open\u001b[0;34m(cls, filename, mode, format, group, clobber, diskless, persist, auto_complex, lock, lock_maker, autoclose)\u001b[0m\n\u001b[1;32m    448\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_complex\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m auto_complex\n\u001b[1;32m    449\u001b[0m manager \u001b[38;5;241m=\u001b[39m CachingFileManager(\n\u001b[1;32m    450\u001b[0m     netCDF4\u001b[38;5;241m.\u001b[39mDataset, filename, mode\u001b[38;5;241m=\u001b[39mmode, kwargs\u001b[38;5;241m=\u001b[39mkwargs\n\u001b[1;32m    451\u001b[0m )\n\u001b[0;32m--> 452\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmanager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mautoclose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautoclose\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/ewatercycle2/lib/python3.12/site-packages/xarray/backends/netCDF4_.py:393\u001b[0m, in \u001b[0;36mNetCDF4DataStore.__init__\u001b[0;34m(self, manager, group, mode, lock, autoclose)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_group \u001b[38;5;241m=\u001b[39m group\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;241m=\u001b[39m mode\n\u001b[0;32m--> 393\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mds\u001b[49m\u001b[38;5;241m.\u001b[39mdata_model\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mds\u001b[38;5;241m.\u001b[39mfilepath()\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_remote \u001b[38;5;241m=\u001b[39m is_remote_uri(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_filename)\n",
      "File \u001b[0;32m/opt/conda/envs/ewatercycle2/lib/python3.12/site-packages/xarray/backends/netCDF4_.py:461\u001b[0m, in \u001b[0;36mNetCDF4DataStore.ds\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mds\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 461\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_acquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/ewatercycle2/lib/python3.12/site-packages/xarray/backends/netCDF4_.py:455\u001b[0m, in \u001b[0;36mNetCDF4DataStore._acquire\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_acquire\u001b[39m(\u001b[38;5;28mself\u001b[39m, needs_lock\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m--> 455\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_manager\u001b[38;5;241m.\u001b[39macquire_context(needs_lock) \u001b[38;5;28;01mas\u001b[39;00m root:\n\u001b[1;32m    456\u001b[0m         ds \u001b[38;5;241m=\u001b[39m _nc4_require_group(root, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_group, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode)\n\u001b[1;32m    457\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ds\n",
      "File \u001b[0;32m/opt/conda/envs/ewatercycle2/lib/python3.12/contextlib.py:137\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt yield\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/ewatercycle2/lib/python3.12/site-packages/xarray/backends/file_manager.py:199\u001b[0m, in \u001b[0;36mCachingFileManager.acquire_context\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;129m@contextlib\u001b[39m\u001b[38;5;241m.\u001b[39mcontextmanager\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21macquire_context\u001b[39m(\u001b[38;5;28mself\u001b[39m, needs_lock\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    198\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Context manager for acquiring a file.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 199\u001b[0m     file, cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_acquire_with_cache_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mneeds_lock\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    201\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m file\n",
      "File \u001b[0;32m/opt/conda/envs/ewatercycle2/lib/python3.12/site-packages/xarray/backends/file_manager.py:217\u001b[0m, in \u001b[0;36mCachingFileManager._acquire_with_cache_info\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    215\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    216\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode\n\u001b[0;32m--> 217\u001b[0m file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_opener\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;66;03m# ensure file doesn't get overridden when opened again\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32msrc/netCDF4/_netCDF4.pyx:2470\u001b[0m, in \u001b[0;36mnetCDF4._netCDF4.Dataset.__init__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/netCDF4/_netCDF4.pyx:2107\u001b[0m, in \u001b[0;36mnetCDF4._netCDF4._ensure_nc_success\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno -70] NetCDF: DAP server error: 'https://opendap.4tu.nl/thredds/dodsC/data2/djht/ca13056c-c347-4a27-b320-930c2a4dd207/1/lamah.nc'"
     ]
    }
   ],
   "source": [
    "# option one: generate forcing data\n",
    "camelsgb_forcing = ewatercycle.forcing.sources['CaravanForcing'].generate(\n",
    "    start_time=experiment_start_date,\n",
    "    end_time=experiment_end_date,\n",
    "    directory=forcing_path_caravan,\n",
    "    basin_id=camelsgb_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35875c4f-d5f0-465c-b349-82580dbd71bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # option two or three: load data that you or someone else generated previously\n",
    "# camelsgb_forcing = ewatercycle.forcing.sources['CaravanForcing'].load(directory=prepared_forcing_path_caravan_central)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b696cee-5dbe-40de-bafb-6346f6265ff3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'camelsgb_forcing' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mcamelsgb_forcing\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'camelsgb_forcing' is not defined"
     ]
    }
   ],
   "source": [
    "print(camelsgb_forcing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc974bcb-5489-4af6-8cf9-cc0299adab97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#quick plot of the discharge data. \n",
    "ds_forcing = xr.open_mfdataset([camelsgb_forcing['Q'],camelsgb_forcing['pr'],camelsgb_forcing['evspsblpot']])\n",
    "ds_forcing[\"Q\"].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d14bd1-ec6b-49fb-a70f-1b64b0688029",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_forcing[\"pr\"].plot(label = 'precipitation')\n",
    "ds_forcing[\"evspsblpot\"].plot(label = 'potential evaporation')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4787c692-3f9c-402b-9b48-93daeeb47926",
   "metadata": {},
   "source": [
    "## Load parameters from calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4557dc0-b086-4230-8368-ef9b91cb2711",
   "metadata": {},
   "source": [
    "The HBV model contains five \"stores\" where the water is stored and nine parameters that control the flow between those stores and in and out of the model.\n",
    "We have already calibrated the model for our region of choice and hardcoded those below. If you have changed the region and have calibrated for your region, you need to point to your calibration results here by uncommenting the load statement and pointing to the right file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324ab8b0-947b-4116-84d9-e476d46ddd98",
   "metadata": {},
   "source": [
    "For the storages we can specify an array of starting values. If you don't the model will start 'empty' and needs some timesteps to 'fill up'. Especially for the rootzone storage it helps to not start empty. Note that all units are in mm: <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551a87c1-eba8-4167-9d70-5b16a323e218",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9e8f254e3dd78daa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#we use the same initial conditions for all models in the ensemble\n",
    "\n",
    "#               Si,  Su, Sf, Ss, Sp\n",
    "s_0 = np.array([0,  100,  0,  5, 0])\n",
    "S_names = [\"Interception storage\", \"Unsaturated Rootzone Storage\", \"Fastflow storage\", \"Groundwater storage\"]\n",
    "\n",
    "#the names of the parameters are (luckily ;-) ) also constant for all models\n",
    "p_names = [\"$I_{max}$\",  \"$C_e$\",  \"$Su_{max}$\", \"Î²\",  \"$P_{max}$\",  \"$T_{lag}$\",   \"$K_f$\",   \"$K_s$\", \"FM\"]\n",
    "param_names = [\"Imax\",\"Ce\",  \"Sumax\", \"beta\",  \"Pmax\",  \"Tlag\",   \"Kf\",   \"Ks\", \"FM\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459d6774-27c9-4530-b174-6d05c0464054",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The number of ensemble members in our ensemble\n",
    "N = 100\n",
    "\n",
    "# p_min_initial= np.array([0,   0.2,  40,    .5,   .001,   1,     .01,  .0001,  .01])\n",
    "# p_max_initial = np.array([8,    1,  800,   4,    .3,     10,    .1,   .01,  0.5])\n",
    "\n",
    "p_min_initial= np.array([0,   0.2,  40,    0.5,   1,   1,     0.01,  0.0001,  0.01])\n",
    "p_max_initial = np.array([50,    1.5,  3000,   30,    4,     50,    3,   3,  5])\n",
    "\n",
    "parameters = np.zeros([9, N])\n",
    "\n",
    "#here I use np.linspace to make a linear interpolation between the minimum and maximum parameters. Realize that this means that the first model will\n",
    "#get all low parameters and the last model will get all high parameters. This can be done much smarter.\n",
    "for param in range(9):\n",
    "    parameters[param,:] = np.linspace(p_min_initial[param],p_max_initial[param],N)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16dacb75-0cc4-47c4-b994-c067ff899650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(parameters.shape)  # Should be (expected_num_params, N)\n",
    "print(parameters)\n",
    "# type(parameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91958ee-9f01-42e4-8b88-addc0f03e9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Zelf: Random Grid Search\n",
    "# import itertools\n",
    "# N = 100\n",
    "# parameters = np.zeros([9, N])\n",
    "# ensemble = []\n",
    "# combinaties = itertools.product(*parameters)\n",
    "# for counter in range(N):\n",
    "#     parameters[ = np.random.uniform(p_min_initial, p_max_initial)\n",
    "\n",
    "#    # print(counter, \":\", random_params)\n",
    "#     if counter % 10 == 0:\n",
    "#        print(counter)\n",
    "    \n",
    "#     ensemble.append(ewatercycle.models.HBVLocal(forcing=camelsgb_forcing))\n",
    "#     config_file, _ = ensemble[counter].setup(\n",
    "#                             parameters = random_params,\n",
    "#                             initial_storage=s_0,\n",
    "#                             cfg_dir = \"configFiles/hbv_ensembleMember_\" + str(counter),\n",
    "#                                )\n",
    "#     ensemble[counter].initialize(config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d11894f-9338-40f2-87a3-5105404d2915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Zelf\n",
    "# import itertools\n",
    "\n",
    "# ensemble = []\n",
    "# combinaties = itertools.product(*parameters)\n",
    "# for counter, comb in enumerate(combinaties):\n",
    "#     if counter % 10 == 0:\n",
    "#         print(counter)\n",
    "#     ensemble.append(ewatercycle.models.HBVLocal(forcing=camelsgb_forcing))\n",
    "#     config_file, _ = ensemble[counter].setup(\n",
    "#                             parameters = list(comb),\n",
    "#                             initial_storage=s_0,\n",
    "#                             cfg_dir = \"configFiles/hbv_ensembleMember_\" + str(counter),\n",
    "#                                )\n",
    "#     ensemble[counter].initialize(config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b6c8a6-b568-4e95-91e3-23c03166c188",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "\n",
    "parameters = np.random.uniform(p_min_initial[:, np.newaxis], p_max_initial[:, np.newaxis], (len(p_min_initial), N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da80fed-40a5-4c64-89c5-d0a33fc0481d",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2a93492c16132434",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ensemble = []\n",
    "\n",
    "# for counter in range(N): \n",
    "#     ensemble.append(ewatercycle.models.HBVLocal(forcing=camelsgb_forcing))\n",
    "#     config_file, _ = ensemble[counter].setup(\n",
    "#                             parameters = ','.join([str(p) for p in parameters[:,counter]]),\n",
    "#                             initial_storage=','.join([str(s) for s in s_0]),\n",
    "#                             cfg_dir = \"configFiles/hbv_ensembleMember_\" + str(counter),\n",
    "#                                )\n",
    "#     ensemble[counter].initialize(config_file)\n",
    "\n",
    "for counter in range(N): \n",
    "    ensemble.append(ewatercycle.models.HBVLocal(forcing=camelsgb_forcing))\n",
    "    config_file, _ = ensemble[counter].setup(\n",
    "                            parameters = parameters[:,counter],\n",
    "                            initial_storage=s_0,\n",
    "                            cfg_dir = \"configFiles/hbv_ensembleMember_\" + str(counter),\n",
    "                               )\n",
    "    ensemble[counter].initialize(config_file)\n",
    "\n",
    "\n",
    "# for counter in range(N): \n",
    "#     ensemble.append(ewatercycle.models.HBVLocal(forcing=camelsgb_forcing))\n",
    "#     config_file, _ = ensemble[counter].setup(\n",
    "#     parameters=list(parameters[:, counter]),  # Pass as a list\n",
    "#     initial_storage=list(s_0),  # Pass as a list\n",
    "#     cfg_dir=f\"configFiles/hbv_ensembleMember_{counter}\",\n",
    "# )\n",
    "#     ensemble[counter].initialize(config_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913f09ce-8d5a-4423-aa87-1a58f702bd36",
   "metadata": {},
   "source": [
    "## Observations and objective function\n",
    "\n",
    "We will compare each model output to observations and we need some sort of objective function to judge if the output is any good and thus determine which parameters are good for this region. I provide a basic (bad!) objective function here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f21434-bbbc-43c6-882a-8e7a4b8e25a4",
   "metadata": {},
   "source": [
    "Here we define the objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401076d6-1e3b-4148-9430-6f98663c454a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calibrationObjective(modelOutput, observation, start_calibration, end_calibration):\n",
    "    # a function that takes in two dataFrames, interpolates the model output to the\n",
    "    # observations and calculates the average absolute difference between the two.\n",
    "\n",
    "    #combine the two in one dataFrame\n",
    "    hydro_data = pd.concat([modelOutput.reindex(observation.index, method = 'ffill'), observation], axis=1)\n",
    "\n",
    "    #only select the calibration period\n",
    "    hydro_data = hydro_data[hydro_data.index > pd.to_datetime(pd.Timestamp(start_calibration).date())]\n",
    "    hydro_data = hydro_data[hydro_data.index < pd.to_datetime(pd.Timestamp(end_calibration).date())]\n",
    "\n",
    "    #calculate mean absolute difference\n",
    "\n",
    "    #diff = hydro_data['Q'] - hydro_data['model output']\n",
    "\n",
    "    #in kwadaat zodat pieken zwaarder wegen\n",
    "    diff = (hydro_data['Q'] - hydro_data['model output']) ** 2\n",
    "    absDiff = np.abs(diff)\n",
    "    meanAbsDiff = np.mean(absDiff)\n",
    "\n",
    "    return meanAbsDiff\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5555847c-8f8b-4df4-9204-f07c2eadaa38",
   "metadata": {},
   "source": [
    "Now we run the entire ensemble. Note that in theory this loop can be run in parallel. If you have access to many core (or a supercomputer), this loop can be speed up considerably! For HBV this is not really a problem, but when doing calibration with larger models, this is a must."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f89386-8d4a-4bc2-b1c0-f7843e7f921c",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0dd66560cf39beeb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#an object to show a progress bar, since this can take a while:\n",
    "f = IntProgress(min=0, max=N) # instantiate the bar\n",
    "display(f) # display the bar\n",
    "\n",
    "#converting the observations to a pandas object, because Rolf is stupid\n",
    "Q_pandas = ds_forcing[\"Q\"].to_dataframe()\n",
    "\n",
    "#an empty array to store the results in\n",
    "objectives = []\n",
    "\n",
    "#loop over all ensemble members\n",
    "for ensembleMember in ensemble:\n",
    "    Q_m = []\n",
    "    time = []\n",
    "    while ensembleMember.time < ensembleMember.end_time:\n",
    "        ensembleMember.update()\n",
    "        discharge_this_timestep = ensembleMember.get_value(\"Q\")\n",
    "        Q_m.append(discharge_this_timestep[0])\n",
    "        time.append(pd.Timestamp(ensembleMember.time_as_datetime.date()))\n",
    "\n",
    "    #calculate the objective function \n",
    "    discharge_dataframe = pd.DataFrame({'model output': Q_m}, index=pd.to_datetime(time))\n",
    "    objective_this_model = calibrationObjective(discharge_dataframe,Q_pandas[\"Q\"],calibration_start_time,calibration_end_time)\n",
    "    objectives.append(objective_this_model)\n",
    "\n",
    "    #it is good practice to remove any variable you don't need anymore to save memory.\n",
    "    del Q_m, time, discharge_dataframe, objective_this_model\n",
    "   \n",
    "    #update progress bar\n",
    "    f.value += 1\n",
    "\n",
    "print(objectives)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84c97de-31d1-4f86-bb80-8988df6d5ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(len(objectives) * 0.1)\n",
    "\n",
    "lowest_indices = np.argsort(objectives)[:n]  # Sorteert en pakt de eerste n indices\n",
    "type(lowest_indices)\n",
    "\n",
    "# Stap 3: Haal de bijbehorende arrays op uit data_matrix\n",
    "lowest_arrays = parameters[:, lowest_indices]\n",
    "\n",
    "objectives = np.array(objectives)\n",
    "# Print resultaten\n",
    "print(\"Laagste 10% waarden in objectives:\\n\", objectives[lowest_indices])\n",
    "print(\"\\nBijbehorende arrays uit data_matrix:\\n\", lowest_arrays)\n",
    "\n",
    "# # Sorteer de lijst en pak de eerste n elementen\n",
    "# lowest_10_percent = sorted(objectives)[:n]\n",
    "\n",
    "# print(lowest_10_percent)\n",
    "\n",
    "# best_parameters = parameters[:, lowest_10_percent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d658d2de-378a-400f-beff-12ad42d5f7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bereken de minimum en maximum waarden per rij\n",
    "p_min_initial = np.min(lowest_arrays, axis=0)  # Kleinste per rij\n",
    "p_max_initial = np.max(lowest_arrays, axis=0)  # Grootste per rij\n",
    "\n",
    "# Print resultaten\n",
    "print(\"Minimumwaarden per rij:\\n\", p_min_initial)\n",
    "print(\"\\nMaximumwaarden per rij:\\n\", p_max_initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79130a0b-0fc4-40d8-8ae5-da7681916f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "\n",
    "parameters = np.random.uniform(p_min_initial[:, np.newaxis], p_max_initial[:, np.newaxis], (len(p_min_initial), N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e4f26e-3171-422e-9b40-d305eaa8f80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = []\n",
    "\n",
    "# for counter in range(N): \n",
    "#     ensemble.append(ewatercycle.models.HBVLocal(forcing=camelsgb_forcing))\n",
    "#     config_file, _ = ensemble[counter].setup(\n",
    "#                             parameters = ','.join([str(p) for p in parameters[:,counter]]),\n",
    "#                             initial_storage=','.join([str(s) for s in s_0]),\n",
    "#                             cfg_dir = \"configFiles/hbv_ensembleMember_\" + str(counter),\n",
    "#                                )\n",
    "#     ensemble[counter].initialize(config_file)\n",
    "\n",
    "for counter in range(N): \n",
    "    ensemble.append(ewatercycle.models.HBVLocal(forcing=camelsgb_forcing))\n",
    "    config_file, _ = ensemble[counter].setup(\n",
    "                            parameters = parameters[:,counter],\n",
    "                            initial_storage=s_0,\n",
    "                            cfg_dir = \"configFiles/hbv_ensembleMember_\" + str(counter),\n",
    "                               )\n",
    "    ensemble[counter].initialize(config_file)\n",
    "\n",
    "\n",
    "# for counter in range(N): \n",
    "#     ensemble.append(ewatercycle.models.HBVLocal(forcing=camelsgb_forcing))\n",
    "#     config_file, _ = ensemble[counter].setup(\n",
    "#     parameters=list(parameters[:, counter]),  # Pass as a list\n",
    "#     initial_storage=list(s_0),  # Pass as a list\n",
    "#     cfg_dir=f\"configFiles/hbv_ensembleMember_{counter}\",\n",
    "# )\n",
    "#     ensemble[counter].initialize(config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a282360c-8c3c-4a35-8692-2ce5ab8ebc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# N = 100  # Aantal te genereren oplossingen per iteratie\n",
    "# num_iterations = 10  # Hoeveel iteraties je wilt uitvoeren\n",
    "\n",
    "# # Stap 2: Iteratief proces\n",
    "# # current_population = np.random.uniform(p_min, p_max, (num_samples, len(p_min)))\n",
    "# parameters = np.random.uniform(p_min_initial[:, np.newaxis], p_max_initial[:, np.newaxis], (len(p_min_initial), N))\n",
    "\n",
    "# for iteration in range(num_iterations):\n",
    "#     objectives = []\n",
    "\n",
    "# #loop over all ensemble members\n",
    "#     for ensembleMember in ensemble:\n",
    "#         Q_m = []\n",
    "#         time = []\n",
    "#         while ensembleMember.time < ensembleMember.end_time:\n",
    "#             ensembleMember.update()\n",
    "#             discharge_this_timestep = ensembleMember.get_value(\"Q\")\n",
    "#             Q_m.append(discharge_this_timestep[0])\n",
    "#             time.append(pd.Timestamp(ensembleMember.time_as_datetime.date()))\n",
    "\n",
    "#     #calculate the objective function \n",
    "#     discharge_dataframe = pd.DataFrame({'model output': Q_m}, index=pd.to_datetime(time))\n",
    "#     objective_this_model = calibrationObjective(discharge_dataframe,Q_pandas[\"Q\"],calibration_start_time,calibration_end_time)\n",
    "#     objectives.append(objective_this_model)\n",
    "\n",
    "#     #it is good practice to remove any variable you don't need anymore to save memory.\n",
    "#     del Q_m, time, discharge_dataframe, objective_this_model\n",
    "    \n",
    "#     # Stap 3: Bereken een fictieve 'objective score' (kan je vervangen met jouw functie)\n",
    "#     # objectives = np.sum(parameters, axis=1)  # Voorbeeld: som van de elementen\n",
    "\n",
    "#     # Stap 4: Selecteer de beste 10%\n",
    "#     n = int(len(objectives) * 0.1)\n",
    "#     best_indices = np.argsort(objectives)[:n]  # Indexen van laagste 10%\n",
    "#     print((best_indices))\n",
    "#     best_solutions = objectives[best_indices]  # Selecteer bijbehorende arrays\n",
    "#     print(best_solutions)\n",
    "\n",
    "#     # Stap 5: Bereken min en max per kolom uit de beste oplossingen\n",
    "#     new_p_min = np.min(best_solutions, axis=0)\n",
    "#     new_p_max = np.max(best_solutions, axis=0)\n",
    "\n",
    "#     # Stap 6: Genereer nieuwe populatie tussen nieuwe min-max\n",
    "#     parameters = np.random.uniform(new_p_min, new_p_max, (num_samples, len(p_min_initial)))\n",
    "\n",
    "#     print(f\"Iteratie {iteration + 1}: Nieuwe min-max ranges berekend\")\n",
    "#     print(\"Nieuwe min waarden:\", new_p_min)\n",
    "#     print(\"Nieuwe max waarden:\", new_p_max)\n",
    "#     print(\"-\" * 50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e476346-a5cd-4d27-bc2a-6e59b071ea20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#finaly, just like before, we remove the models themselves to save up space and memory.\n",
    "for ensembleMember in ensemble:\n",
    "    ensembleMember.finalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b583adc-ca52-4971-b041-e9786b3232ad",
   "metadata": {},
   "source": [
    "## Analyse results\n",
    "We now have objective function results for all ensemble members! Let's make some plots!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f71d06-2034-4a33-a1af-78d983d3d5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea60bdd9-631b-46a0-a6d4-b9f86a5e333f",
   "metadata": {},
   "outputs": [],
   "source": [
    "xFigNr = 2\n",
    "yFigNr = 4\n",
    "\n",
    "fig, axs = plt.subplots(xFigNr, yFigNr,figsize = (15,15))\n",
    "\n",
    "for xFig in range(xFigNr):\n",
    "    for yFig in range(yFigNr):\n",
    "        paramCounter = xFig*yFigNr + yFig\n",
    "        axs[xFig,yFig].plot(parameters[paramCounter,:],objectives,'.')\n",
    "        axs[xFig,yFig].set_title(p_names[paramCounter])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53581554-4e73-417f-a1af-a8c2830f95d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's also show the minimal values:\n",
    "parameters_minimum_index = np.argmin(np.array(objectives))\n",
    "\n",
    "objective_minimum = np.min(objectives)\n",
    "\n",
    "parameters_minimum = parameters[:,parameters_minimum_index]\n",
    "\n",
    "print(\"Best parameter index:\", parameters_minimum_index)\n",
    "print(\"Best parameter set:\", parameters_minimum)\n",
    "print(\"Objective: \", objective_minimum)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004d9326-d152-4793-ac7b-b46f8090e10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_min_initial= np.array([0,   0.2,  40,    0.5,   0.001,   1,     0.01,  0.0001,  0.01])\n",
    "p_max_initial = np.array([10,    3,  1000,   6,    .6,     15,    .3,   .03,  0.8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449bd715-b7b8-4545-993d-b92210cfa6fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
