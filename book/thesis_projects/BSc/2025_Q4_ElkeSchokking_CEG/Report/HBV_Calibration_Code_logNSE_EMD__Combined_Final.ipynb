{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec436ed0-14fc-4985-bd19-5eb99fc2e04c",
   "metadata": {},
   "source": [
    "# The HBV Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1086322-b387-49f7-9614-83a4c36362ca",
   "metadata": {},
   "source": [
    "For the simulation of anticipated future discharge, the Hydrologiska Byråns Vattenbalansavdelning (HBV) model was selected. Developed by Sten Bergström (in 1992) (Wetterhall, 2017), the HBV framework is classified as a semi-distributed conceptual rainfall–runoff model. In this approach, a catchment is represented by a network of interconnected storage reservoirs that emulate the hydrological processes governing water movement. Calibration to diverse catchments is achieved via adjustment of nine model parameters. The HBV model requires three meteorological forcing inputs—precipitation, air temperature and potential evapotranspiration—to drive its internal mass-balance computations. Each parameter modulates a specific hydrological process, enabling the HBV model to reproduce observed hydrographs across a range of climatic and physiographic "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec61532f-b32a-4d03-b349-74aa6b34d8c5",
   "metadata": {},
   "source": [
    "The nine calibration parameters are as follows:\n",
    "\n",
    "*Table 3.1: HBV model parameters*\n",
    "\n",
    "| **Parameter**        | **Description**                                          |\n",
    "|----------------------|----------------------------------------------------------|\n",
    "| $I_{\\text{max}}$     | Maximum soil moisture storage capacity                   |\n",
    "| $C_{\\text{e}}$       | Evapotranspiration correction factor                     |\n",
    "| $S_{\\text{u,max}}$   | Upper zone storage capacity                              |\n",
    "| $\\beta$              | Nonlinearity exponent governing soil moisture runoff     |\n",
    "| $P_{\\text{max}}$     | Threshold precipitation for snow accumulation            |\n",
    "| $T_{\\text{lag}}$     | Temperature threshold for snowmelt delay                 |\n",
    "| $K_{\\text{f}}$       | Recession coefficient for the upper storage reservoir    |\n",
    "| $K_{\\text{s}}$       | Recession coefficient for the lower storage reservoir    |\n",
    "| $F_{\\text{M}}$       | Fractional melt coefficient for snowpack                 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdd451b-304b-45c0-aa09-28409caa732f",
   "metadata": {},
   "source": [
    "<figure>\n",
    "  <img src=\"HBV.png\" width=\"600\">\n",
    "  <figcaption><b>Figure 3.1:</b> The HBV model <i>(Hrachowitz, n.d.)</i></figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f00ef72-d4c4-4a01-94ea-69e342a7faff",
   "metadata": {},
   "source": [
    "## Model Calibration Methodology\n",
    "The HBV model uses simplified representations of hydrological processes, with parameters that often cannot be directly measured. To ensure the model accurately reflects observed discharge patterns, an automated calibration approach is applied. This approach focuses on accuracy during low-flow conditions (below 500 m³/s) and on the correct timing of these low-flow events.\n",
    "\n",
    "The calibration combines a multi-criteria objective function with a structured, step-by-step procedure. This leads to the identification of optimized parameter sets that enhance the model’s performance (Yilmaz et al., 2010)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee5c51d-893f-488f-889c-8c95e9dec5ee",
   "metadata": {},
   "source": [
    "## Setup and Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd089a07-e205-4df7-b638-3f03693fabd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#some general python and eWaterCycle libraries need to be imported\n",
    "%matplotlib inline\n",
    "# General python\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "import xarray as xr\n",
    "from scipy.stats import wasserstein_distance\n",
    "\n",
    "# Niceties\n",
    "from rich import print\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# General eWaterCycle\n",
    "import ewatercycle\n",
    "import ewatercycle.models\n",
    "import ewatercycle.forcing\n",
    "from ewatercycle.forcing import sources\n",
    "from ewatercycle.models import HBV\n",
    "\n",
    "# Optional: Data Assimilation\n",
    "# If not installed, uncomment below to install\n",
    "# !pip install ewatercycle-da\n",
    "from ewatercycle_DA import DA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d65bbc6-038e-482c-8ac9-c47187638374",
   "metadata": {},
   "source": [
    "## Choose region and time period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf1a189d-d7e8-4902-a9b6-7591b80c4ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # name of the catchment\n",
    "# basin_name = \"FR000119\"\n",
    "\n",
    "# # defining dates for calibration\n",
    "# experiment_start_date = \"2013-11-26T00:00:00Z\"\n",
    "# experiment_end_date = \"2019-12-31T00:00:00Z\"\n",
    "\n",
    "# #Define Catchment Area\n",
    "# shapefile_path = Path.home() / \"BEP-Elke\" / \"book\" / \"thesis_projects\" / \"BSc\" / \"2025_Q4_ElkeSchokking_CEG\" / \"work in progress\" / \"ShapefilesFR000119\" / \"FR000119.shp\"\n",
    "\n",
    "# #check\n",
    "# catchment = gpd.read_file(shapefile_path)\n",
    "# catchment = catchment.to_crs(epsg=3035)\n",
    "# catchment[\"area_km2\"] = catchment.geometry.area / 1e6  \n",
    "# basin_area = catchment[\"area_km2\"].sum()\n",
    "# catchment.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5485562-e12b-466f-b42c-69a56b96ac5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Location forcing files in home directory\n",
    "# forcing_path = Path.home() / \"forcing\" / \"FR000119\"/\"ERA5\"\n",
    "# forcing_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2235d1a-3eb9-427d-b6e5-50848565fedd",
   "metadata": {},
   "source": [
    "## Generate ERA 5 Forcing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8abd4fc5-f7ce-496b-af4f-fecdc30a4be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "forcing_dir = Path(\"/home/elke/BEP-Elke/book/thesis_projects/BSc/2025_Q4_ElkeSchokking_CEG/work in progress/esmvaltool_output/ewcrep5yvtlv4f_20250525_104347/work/diagnostic/script\")\n",
    "ERA5_forcing = sources[\"LumpedMakkinkForcing\"].load(directory=forcing_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92374f0a-73da-4f37-97dd-831cd33aed07",
   "metadata": {},
   "source": [
    "## Defining historical data from eStreams\n",
    "The original CSV file had some formatting and encoding issues—like strange quotation marks and all the data crammed into a single column—which made it impossible to load with the usual pandas.read_csv() method. To work around this, I used a custom parser to manually extract the dates and discharge values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "094c39b1-6e56-4394-9ecc-a23f3e48f5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually parse the file\n",
    "dates = []\n",
    "discharges = []\n",
    "\n",
    "with open(\"A3550050.csv\", \"r\", encoding=\"utf-8\") as f:\n",
    "    next(f)\n",
    "    for line in f:\n",
    "        parts = line.replace('\"\"', '\"').strip().split(',\"')\n",
    "        if len(parts) >= 2:\n",
    "            date_str = parts[0].strip('\"')\n",
    "            discharge_str = parts[1].strip('\"')\n",
    "            try:\n",
    "                dates.append(pd.to_datetime(date_str))\n",
    "                discharges.append(float(discharge_str))\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "discharge_series = pd.Series(discharges, index=dates, name=\"Discharge (m³/s)\")\n",
    "Q_obs = discharge_series[experiment_start_date:experiment_end_date]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4b5b54-8161-4f90-846b-78d4365976dc",
   "metadata": {},
   "source": [
    "## Ensemble Initialization and Parameter Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb0070d-6ffd-465a-b54c-cfd8afc00d90",
   "metadata": {},
   "source": [
    "### Calibration Algorithm\n",
    "Model calibration using the ensemble method involves creating an ensemble of multiple models, this method is taken from eWatercycle and Hut, 2025. Instead of a single one. Each \"ensemble member\" is assigned its own unique set of parameters. These parameters are created by generating random values within predefined ranges. All models within this ensemble can then be run simultaneously with a single command, like ensemble.update(). After these runs, an objective function is applied to each model's output to quantify its \"goodness of fit\" by comparing it to the observed dicharge data. The best set of parameters is then identified as the combination that yields the lowest objective function score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30616e6f-be53-41da-9ade-6c2dda6cd5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define HBV parameter bounds and names\n",
    "param_names = [\"Imax\", \"Ce\", \"Sumax\", \"Beta\", \"Pmax\", \"Tlag\", \"Kf\", \"Ks\", \"FM\"]\n",
    "p_min = np.array([0.0,  0.2,   40.0,  0.5,   0.001,   1.0,   0.01,  0.0001,  0.01])\n",
    "p_max = np.array([8.0,  1.0,  800.0,  4.0,   0.3,    10.0,   0.1,   0.01,   10.0])\n",
    "\n",
    "n_particles = 1000  # ensemble size\n",
    "# Sample random parameters for each particle within bounds\n",
    "parameters = np.zeros((len(param_names), n_particles))\n",
    "for j in range(len(param_names)):\n",
    "    parameters[j, :] = np.random.uniform(p_min[j], p_max[j], size=n_particles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd97535d-53ea-4a54-aa37-56c8d3ddfaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ensemble and initialize each member with HBV model and unique parameters\n",
    "ensemble = DA.Ensemble(N=n_particles)\n",
    "ensemble.setup() \n",
    "\n",
    "# Prepare setup arguments for each particle (each gets its parameter set)\n",
    "setup_kwargs_list = [{'parameters': parameters[:, i]} for i in range(n_particles)]\n",
    "\n",
    "# Initialize all ensemble members with the HBVLocal model, forcing data, and parameters\n",
    "ensemble.initialize(model_name=[\"HBVLocal\"] * n_particles,\n",
    "                    forcing=[ERA5_forcing] * n_particles,\n",
    "                    setup_kwargs=setup_kwargs_list) \n",
    "\n",
    "ref_model = ensemble.ensemble_list[0].model\n",
    "\n",
    "# Generate config file with first parameter set\n",
    "config_file, _ = ref_model.setup(parameters=parameters[:, 0])\n",
    "\n",
    "# Initialize model\n",
    "ref_model.initialize(config_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebd6bea-6539-4448-90d7-2b48b4d78a7c",
   "metadata": {},
   "source": [
    "## Running the Ensemble Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92a3452e-3658-45a4-84c6-7f80f66de40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine number of time steps in the simulation period\n",
    "n_timesteps = int((ref_model.end_time - ref_model.start_time) / ref_model.time_step)\n",
    "\n",
    "time_index = []           # list to store timestamps for each time step\n",
    "ensemble_Q_outputs = []   # list to store discharge arrays for each time step\n",
    "\n",
    "for step in range(n_timesteps):\n",
    "    # Record current model time\n",
    "    current_time = pd.Timestamp(ref_model.time_as_datetime.date())\n",
    "    time_index.append(current_time)\n",
    "    \n",
    "    # Advance all models by one time step and collect their discharge values\n",
    "    ensemble.update()  # update all ensemble members by one step\n",
    "    Q_values = np.array(ensemble.get_value(\"Q\")).flatten()  # discharge of all particles\n",
    "    Q_values_m3s = Q_values * basin_area * 1000 / 86400 #convert to m3/s\n",
    "    ensemble_Q_outputs.append(Q_values_m3s)\n",
    "\n",
    "ensemble.finalize()\n",
    "\n",
    "# Convert collected outputs to a DataFrame\n",
    "Q_array = np.array(ensemble_Q_outputs)        # shape: (n_timesteps, n_particles)\n",
    "df_ensemble = pd.DataFrame(\n",
    "    data=Q_array, \n",
    "    index=pd.DatetimeIndex(time_index), \n",
    "    columns=[f\"particle_{i}\" for i in range(n_particles)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889ce629-d9c6-46b3-90c5-6a6ccbb808cc",
   "metadata": {},
   "source": [
    "## Defining the Calibration Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66de76b-d14a-48d1-bef5-e49efded04b4",
   "metadata": {},
   "source": [
    "### Objective Functions\n",
    "To accurately represent both the size and timing of low-flow events, a multi-criteria approach is used. The use of multiple performance measures helps to achieve a better balance between the number of calibration parameters and the quantity of the available observational data. This makes the parameter estimation more reliable than when using a single measure (Yilmaz et al., 2010). Two specific objective functions are selected for their ability to reflect low-flow conditions and timing accuracy.\n",
    "\n",
    "    1. For Low-Flow Magnitude and General Fit\n",
    "To make sure the low-flow values are most accurate the model performance will be evaluated using the Nash–Sutcliffe Efficiency (NSE). It will be applied to log-transformed discharge values. The logarithmic transformation increases sensitivity to low-flow conditions (Yilmaz et al., 2010). NSE is based on squared differences. Which causes it to remain numerically stable under transformation and means it is less affected by unit conversions (Santos et al., 2018)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ea1090-6800-47ad-9451-36fcd3d24e50",
   "metadata": {},
   "source": [
    "$$\n",
    "\\log\\text{-NSE} = 1 - \\frac{\\sum_t \\left(\\log Q_{\\text{sim},t} - \\log Q_{\\text{obs},t}\\right)^2}{\\sum_t \\left(\\log Q_{\\text{obs},t} - \\log \\overline{Q}_{\\text{obs}}\\right)^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6409349-bd8b-431b-b70a-5e243920bde5",
   "metadata": {},
   "source": [
    "Where:\n",
    "\n",
    "- $Q_{\\text{sim},t}$ is the simulated discharge at time $t$, where $Q_{\\text{sim},t} > 0$.\n",
    "- $Q_{\\text{obs},t}$ is the observed discharge at time $t$, where $Q_{\\text{obs},t} > 0$.\n",
    "- $\\overline{Q}_{\\text{obs}}$ is the mean of the log-transformed observed discharges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7a8640-ff4e-48bb-ba22-55d9542964e4",
   "metadata": {},
   "source": [
    "> **Note:** Only positive discharge values are used to ensure the logarithm is defined. A higher log-NSE indicates better model performance, especially under low-flow conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a44ce35-3d57-4ca7-84c7-dbab01a43c38",
   "metadata": {},
   "source": [
    "    2. For Timing and Low-Flow Properties\n",
    "To improve the accuracy of the low-flow discharge timeline, the Earth Mover's Distance (EMD) will be used. It is also known as the Wasserstein Distance (WD). It compares the temporal distributions of droughts between observed and simulated data. By minimizing the EMD, the calibration process aligns the timing of low-flow periods (Ehret & Zehe, 2011)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e95a72-8497-4143-928e-f33979a67073",
   "metadata": {},
   "source": [
    "For discrete distributions $P$ and $Q$, the Earth Mover's Distance (EMD) is defined as:\n",
    "\n",
    "$$\n",
    "\\text{EMD}(P, Q) = \\min_{f_{ij} \\geq 0} \\sum_{i,j} f_{ij} \\, d_{ij}\n",
    "$$\n",
    "\n",
    "Subject to:\n",
    "\n",
    "$$\n",
    "\\sum_j f_{ij} = P_i, \\quad \\sum_i f_{ij} = Q_j\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $P_i$ and $Q_j$ are discrete probability distributions of drought features (e.g., durations or deficits) for observed and simulated data, respectively.\n",
    "- $f_{ij}$ is the amount of \"mass\" moved from bin $i$ in $P$ to bin $j$ in $Q$.\n",
    "- $d_{ij}$ is the temporal distance or difference in drought duration between bins $i$ and $j$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775fee18-f498-46fa-9355-4cbfcb1d0789",
   "metadata": {},
   "source": [
    "#### Multi-Criteria Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d66b95f-fd96-4730-9e93-e650f23d3a26",
   "metadata": {},
   "source": [
    "The two objective functions will be combined using a multi-criteria framework. A weighted sum is used, where each criterion is given a weight to reflect its relative importance. This allows the calibration to balance overall low-flow accuracy with the timing and characteristics of drought events.\n",
    "\n",
    "To combine log-NSE and EMD into a single optimization objective while emphasizing low-flow quantity (not just timing), the following **weighted error function** $J$ is defined:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bd201c-84b7-4187-ba49-ed4727709108",
   "metadata": {},
   "source": [
    "$$\n",
    "J = w_{\\log \\text{NSE}} \\cdot (1 - \\log \\text{NSE}) + w_{\\text{EMD}} \\cdot \\text{EMD}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $(1 - \\log \\text{NSE})$ transforms the log-NSE (normally a maximization metric) into a minimization objective.\n",
    "- $w_{\\log \\text{NSE}}$ and $w_{\\text{EMD}}$ are weights that control the emphasis between low-flow magnitude accuracy and timing.\n",
    "\n",
    "*Weight setting:*\n",
    "\n",
    "Since low-flow magnitude accuracy is the primary focus:\n",
    "\n",
    "$w_{\\log \\text{NSE}} > w_{\\text{EMD}}$\n",
    "\n",
    "In this study the weight setting is as follows:\n",
    "\n",
    "$w_{\\log \\text{NSE}} = 0.7$, $w_{\\text{EMD}} = 0.3$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6309b70-d220-4485-944b-5d50f5ddce9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calibration_objective(simulated_series: pd.Series, observed_series: pd.Series,\n",
    "                          w_lognse: float = 0.7, w_emd: float = 0.3) -> float:\n",
    "    \"\"\"\n",
    "    Calibrate model by combining:\n",
    "    - log-NSE: performance during low flows\n",
    "    - EMD: match drought event characteristics (timing)\n",
    "    Weights w_lognse and w_emd define the importance of each.\n",
    "    Lower total_score indicates better calibration.\n",
    "    \"\"\"\n",
    "\n",
    "    # Align indices\n",
    "    sim = simulated_series.loc[observed_series.index]\n",
    "    obs = observed_series\n",
    "\n",
    "    # 1. log-NSE\n",
    "    sim_pos = sim[sim > 0]\n",
    "    obs_pos = obs[obs > 0]\n",
    "    common_index = sim_pos.index.intersection(obs_pos.index)\n",
    "    sim_log = np.log(sim_pos.loc[common_index])\n",
    "    obs_log = np.log(obs_pos.loc[common_index])\n",
    "    \n",
    "    numerator = np.sum((sim_log - obs_log) ** 2)\n",
    "    denominator = np.sum((obs_log - np.mean(obs_log)) ** 2)\n",
    "    log_nse = 1 - (numerator / denominator) if denominator != 0 else np.inf\n",
    "    error_nse = 1 - log_nse  # to be minimized\n",
    "\n",
    "    # 2. EMD from drought characteristics\n",
    "    obs_droughts = droughts(obs, basin_name=\"obs\", q_crit=500)\n",
    "    sim_droughts = droughts(sim, basin_name=\"sim\", q_crit=500)\n",
    "\n",
    "    obs_durations = pd.DataFrame(obs_droughts)[\"Duration (days)\"].values\n",
    "    sim_durations = pd.DataFrame(sim_droughts)[\"Duration (days)\"].values\n",
    "    obs_deficits = pd.DataFrame(obs_droughts)[\"Max Cumulative Deficit (m3/s)\"].abs().values\n",
    "    sim_deficits = pd.DataFrame(sim_droughts)[\"Max Cumulative Deficit (m3/s)\"].abs().values\n",
    "\n",
    "    emd_duration = wasserstein_distance(obs_durations, sim_durations)\n",
    "    emd_deficit = wasserstein_distance(obs_deficits, sim_deficits)\n",
    "    error_emd = emd_duration + emd_deficit\n",
    "\n",
    "    # Combine with weighted sum\n",
    "    total_score = w_lognse * error_nse + w_emd * error_emd\n",
    "    return total_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb057fe4-718d-4883-8e07-1317e6074517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Q_obs index sample:\", Q_obs.index[:3])\n",
    "# print(\"df_ensemble index sample:\", df_ensemble.index[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920f5d40-e059-477e-9acf-0b9c012f40b6",
   "metadata": {},
   "source": [
    "## Evaluating Performance and Selecting the Best Parameter Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2e947a3-7167-42fe-8a7c-1513982d7bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate objective for each ensemble member\n",
    "scores = []\n",
    "\n",
    "# Same datetime-index\n",
    "Q_obs.index = Q_obs.index.normalize()\n",
    "Q_obs.index = Q_obs.index.tz_localize(None)\n",
    "\n",
    "# Slice only the overlap \n",
    "Q_obs_aligned = Q_obs.loc[df_ensemble.index.min():df_ensemble.index.max()]\n",
    "\n",
    "for i in range(n_particles):\n",
    "    sim_series = df_ensemble[f\"particle_{i}\"]\n",
    "\n",
    "    if sim_series is None or sim_series.empty:\n",
    "        print(f\"Skipping particle_{i} (empty output)\")\n",
    "        scores.append(np.inf)  # Assign a bad score\n",
    "        continue\n",
    "\n",
    "    # Use the correct observation series (make sure Q_obs exists and matches in index)\n",
    "    score = calibration_objective(sim_series, Q_obs_aligned)\n",
    "    scores.append(score)\n",
    "\n",
    "scores = np.array(scores)\n",
    "best_index = np.argmin(scores)              # index of minimum objective value\n",
    "best_score = scores[best_index]             # lowest score\n",
    "best_params = parameters[:, best_index]     # parameter set corresponding to best score\n",
    "\n",
    "print(f\"Best score: {best_score:.3f} (part {best_index})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8eb397-b432-4555-a5c7-ad9b286ec5f2",
   "metadata": {},
   "source": [
    "## Show the Optimal Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9477af8-b5ef-4fbe-8ea3-a1c33a10aef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best objective score: {best_score:.4f}\")\n",
    "\n",
    "# Round best parameters\n",
    "best_params_list = [round(val, 4) for val in best_params]\n",
    "\n",
    "# Define parameter descriptions (matches param_names)\n",
    "param_descriptions = [\n",
    "    \"Maximum intensity\",\n",
    "    \"Coefficient of evaporation\",\n",
    "    \"Field capacity\",\n",
    "    \"Shape coefficient\",\n",
    "    \"Maximum percolation rate\",\n",
    "    \"Time lag\",\n",
    "    \"Fast run-off parameter\",\n",
    "    \"Slow run-off parameter\",\n",
    "    \"Degree-day factor\"\n",
    "]\n",
    "\n",
    "# Build DataFrame\n",
    "df_params = pd.DataFrame({\n",
    "    \"Parameter\": param_names,\n",
    "    \"Description\": param_descriptions,\n",
    "    \"Value\": best_params_list\n",
    "})\n",
    "\n",
    "# Reset index and drop it for clean display\n",
    "df_params.index = [\"\"] * len(df_params)\n",
    "\n",
    "display(df_params.style.set_table_styles([\n",
    "    {\"selector\": \"th\", \"props\": [(\"font-weight\", \"bold\")]}\n",
    "]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
