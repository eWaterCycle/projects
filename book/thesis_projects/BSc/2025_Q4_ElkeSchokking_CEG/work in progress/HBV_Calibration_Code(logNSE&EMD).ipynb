{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec436ed0-14fc-4985-bd19-5eb99fc2e04c",
   "metadata": {},
   "source": [
    "# HBV Calibration Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee5c51d-893f-488f-889c-8c95e9dec5ee",
   "metadata": {},
   "source": [
    "## Setup and Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd089a07-e205-4df7-b638-3f03693fabd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#some general python and eWaterCycle libraries need to be imported\n",
    "%matplotlib inline\n",
    "# General python\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "import xarray as xr\n",
    "from scipy.stats import wasserstein_distance\n",
    "\n",
    "# Niceties\n",
    "from rich import print\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# General eWaterCycle\n",
    "import ewatercycle\n",
    "import ewatercycle.models\n",
    "import ewatercycle.forcing\n",
    "from ewatercycle.forcing import sources\n",
    "from ewatercycle.models import HBV\n",
    "\n",
    "# Optional: Data Assimilation\n",
    "# If not installed, uncomment below to install\n",
    "# !pip install ewatercycle-da\n",
    "from ewatercycle_DA import DA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d65bbc6-038e-482c-8ac9-c47187638374",
   "metadata": {},
   "source": [
    "## Choose region and time period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf1a189d-d7e8-4902-a9b6-7591b80c4ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # name of the catchment\n",
    "# basin_name = \"FR000119\"\n",
    "\n",
    "# # defining dates for calibration\n",
    "# experiment_start_date = \"2013-11-26T00:00:00Z\"\n",
    "# experiment_end_date = \"2019-12-31T00:00:00Z\"\n",
    "\n",
    "# #Define Catchment Area\n",
    "# shapefile_path = Path.home() / \"BEP-Elke\" / \"book\" / \"thesis_projects\" / \"BSc\" / \"2025_Q4_ElkeSchokking_CEG\" / \"work in progress\" / \"ShapefilesFR000119\" / \"FR000119.shp\"\n",
    "\n",
    "# #check\n",
    "# catchment = gpd.read_file(shapefile_path)\n",
    "# catchment = catchment.to_crs(epsg=3035)\n",
    "# catchment[\"area_km2\"] = catchment.geometry.area / 1e6  \n",
    "# basin_area = catchment[\"area_km2\"].sum()\n",
    "# catchment.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5485562-e12b-466f-b42c-69a56b96ac5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Location forcing files in home directory\n",
    "# forcing_path = Path.home() / \"forcing\" / \"FR000119\"/\"ERA5\"\n",
    "# forcing_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2235d1a-3eb9-427d-b6e5-50848565fedd",
   "metadata": {},
   "source": [
    "## Generate ERA 5 Forcing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8abd4fc5-f7ce-496b-af4f-fecdc30a4be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option one: generate forcing:\n",
    "#ERA5_forcing = sources[\"LumpedMakkinkForcing\"].generate(\n",
    "    #dataset=\"ERA5\",\n",
    "    #start_time=experiment_start_date,\n",
    "    #end_time=experiment_end_date,\n",
    "    #shape=shapefile_path,\n",
    "#)\n",
    "\n",
    "# Option two: Load generated (merged) data\n",
    "forcing_dir = Path(\"/home/elke/BEP-Elke/book/thesis_projects/BSc/2025_Q4_ElkeSchokking_CEG/work in progress/esmvaltool_output/ewcrep5yvtlv4f_20250525_104347/work/diagnostic/script\")\n",
    "ERA5_forcing = sources[\"LumpedMakkinkForcing\"].load(directory=forcing_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92374f0a-73da-4f37-97dd-831cd33aed07",
   "metadata": {},
   "source": [
    "## Defining historical data from eStreams\n",
    "The original CSV file had some formatting and encoding issues—like strange quotation marks and all the data crammed into a single column—which made it impossible to load with the usual pandas.read_csv() method. To work around this, I used a custom parser to manually extract the dates and discharge values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "094c39b1-6e56-4394-9ecc-a23f3e48f5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually parse the file\n",
    "dates = []\n",
    "discharges = []\n",
    "\n",
    "with open(\"A3550050.csv\", \"r\", encoding=\"utf-8\") as f:\n",
    "    next(f)\n",
    "    for line in f:\n",
    "        parts = line.replace('\"\"', '\"').strip().split(',\"')\n",
    "        if len(parts) >= 2:\n",
    "            date_str = parts[0].strip('\"')\n",
    "            discharge_str = parts[1].strip('\"')\n",
    "            try:\n",
    "                dates.append(pd.to_datetime(date_str))\n",
    "                discharges.append(float(discharge_str))\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "discharge_series = pd.Series(discharges, index=dates, name=\"Discharge (m³/s)\")\n",
    "Q_obs = discharge_series[experiment_start_date:experiment_end_date]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4b5b54-8161-4f90-846b-78d4365976dc",
   "metadata": {},
   "source": [
    "## Ensemble Initialization and Parameter Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30616e6f-be53-41da-9ade-6c2dda6cd5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define HBV parameter bounds and names\n",
    "param_names = [\"Imax\", \"Ce\", \"Sumax\", \"Beta\", \"Pmax\", \"Tlag\", \"Kf\", \"Ks\", \"FM\"]\n",
    "p_min = np.array([0.0,  0.2,   40.0,  0.5,   0.001,   1.0,   0.01,  0.0001,  0.01])\n",
    "p_max = np.array([8.0,  1.0,  800.0,  4.0,   0.3,    10.0,   0.1,   0.01,   10.0])\n",
    "\n",
    "n_particles = 1000  # ensemble size\n",
    "# Sample random parameters for each particle within bounds\n",
    "parameters = np.zeros((len(param_names), n_particles))\n",
    "for j in range(len(param_names)):\n",
    "    parameters[j, :] = np.random.uniform(p_min[j], p_max[j], size=n_particles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd97535d-53ea-4a54-aa37-56c8d3ddfaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ensemble and initialize each member with HBV model and unique parameters\n",
    "ensemble = DA.Ensemble(N=n_particles)\n",
    "ensemble.setup() \n",
    "\n",
    "# Prepare setup arguments for each particle (each gets its parameter set)\n",
    "setup_kwargs_list = [{'parameters': parameters[:, i]} for i in range(n_particles)]\n",
    "\n",
    "# Initialize all ensemble members with the HBVLocal model, forcing data, and parameters\n",
    "ensemble.initialize(model_name=[\"HBVLocal\"] * n_particles,\n",
    "                    forcing=[ERA5_forcing] * n_particles,\n",
    "                    setup_kwargs=setup_kwargs_list) \n",
    "\n",
    "ref_model = ensemble.ensemble_list[0].model\n",
    "\n",
    "# Generate config file with first parameter set\n",
    "config_file, _ = ref_model.setup(parameters=parameters[:, 0])\n",
    "\n",
    "# Initialize model\n",
    "ref_model.initialize(config_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebd6bea-6539-4448-90d7-2b48b4d78a7c",
   "metadata": {},
   "source": [
    "## Running the Ensemble Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92a3452e-3658-45a4-84c6-7f80f66de40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine number of time steps in the simulation period\n",
    "n_timesteps = int((ref_model.end_time - ref_model.start_time) / ref_model.time_step)\n",
    "\n",
    "time_index = []           # list to store timestamps for each time step\n",
    "ensemble_Q_outputs = []   # list to store discharge arrays for each time step\n",
    "\n",
    "for step in range(n_timesteps):\n",
    "    # Record current model time\n",
    "    current_time = pd.Timestamp(ref_model.time_as_datetime.date())\n",
    "    time_index.append(current_time)\n",
    "    \n",
    "    # Advance all models by one time step and collect their discharge values\n",
    "    ensemble.update()  # update all ensemble members by one step\n",
    "    Q_values = np.array(ensemble.get_value(\"Q\")).flatten()  # discharge of all particles\n",
    "    Q_values_m3s = Q_values * basin_area * 1000 / 86400 #convert to m3/s\n",
    "    ensemble_Q_outputs.append(Q_values_m3s)\n",
    "\n",
    "ensemble.finalize()\n",
    "\n",
    "# Convert collected outputs to a DataFrame\n",
    "Q_array = np.array(ensemble_Q_outputs)        # shape: (n_timesteps, n_particles)\n",
    "df_ensemble = pd.DataFrame(\n",
    "    data=Q_array, \n",
    "    index=pd.DatetimeIndex(time_index), \n",
    "    columns=[f\"particle_{i}\" for i in range(n_particles)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e16ef914-7b58-46ce-9700-fc50c0e44170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df_ensemble.shape)\n",
    "# print(df_ensemble.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889ce629-d9c6-46b3-90c5-6a6ccbb808cc",
   "metadata": {},
   "source": [
    "## Defining the Calibration Objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6309b70-d220-4485-944b-5d50f5ddce9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calibration_objective(simulated_series: pd.Series, observed_series: pd.Series,\n",
    "                          w_lognse: float = 0.7, w_emd: float = 0.3) -> float:\n",
    "    \"\"\"\n",
    "    Calibrate model by combining:\n",
    "    - log-NSE: performance during low flows\n",
    "    - EMD: match drought event characteristics (timing)\n",
    "    Weights w_lognse and w_emd define the importance of each.\n",
    "    Lower total_score indicates better calibration.\n",
    "    \"\"\"\n",
    "\n",
    "    # Align indices\n",
    "    sim = simulated_series.loc[observed_series.index]\n",
    "    obs = observed_series\n",
    "\n",
    "    # 1. log-NSE\n",
    "    sim_pos = sim[sim > 0]\n",
    "    obs_pos = obs[obs > 0]\n",
    "    common_index = sim_pos.index.intersection(obs_pos.index)\n",
    "    sim_log = np.log(sim_pos.loc[common_index])\n",
    "    obs_log = np.log(obs_pos.loc[common_index])\n",
    "    \n",
    "    numerator = np.sum((sim_log - obs_log) ** 2)\n",
    "    denominator = np.sum((obs_log - np.mean(obs_log)) ** 2)\n",
    "    log_nse = 1 - (numerator / denominator) if denominator != 0 else np.inf\n",
    "    error_nse = 1 - log_nse  # to be minimized\n",
    "\n",
    "    # 2. EMD from drought characteristics\n",
    "    obs_droughts = droughts(obs, basin_name=\"obs\", q_crit=500)\n",
    "    sim_droughts = droughts(sim, basin_name=\"sim\", q_crit=500)\n",
    "\n",
    "    obs_durations = pd.DataFrame(obs_droughts)[\"Duration (days)\"].values\n",
    "    sim_durations = pd.DataFrame(sim_droughts)[\"Duration (days)\"].values\n",
    "    obs_deficits = pd.DataFrame(obs_droughts)[\"Max Cumulative Deficit (m3/s)\"].abs().values\n",
    "    sim_deficits = pd.DataFrame(sim_droughts)[\"Max Cumulative Deficit (m3/s)\"].abs().values\n",
    "\n",
    "    emd_duration = wasserstein_distance(obs_durations, sim_durations)\n",
    "    emd_deficit = wasserstein_distance(obs_deficits, sim_deficits)\n",
    "    error_emd = emd_duration + emd_deficit\n",
    "\n",
    "    # Combine with weighted sum\n",
    "    total_score = w_lognse * error_nse + w_emd * error_emd\n",
    "    return total_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb057fe4-718d-4883-8e07-1317e6074517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Q_obs index sample:\", Q_obs.index[:3])\n",
    "# print(\"df_ensemble index sample:\", df_ensemble.index[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920f5d40-e059-477e-9acf-0b9c012f40b6",
   "metadata": {},
   "source": [
    "## Evaluating Performance and Selecting the Best Parameter Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2e947a3-7167-42fe-8a7c-1513982d7bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate objective for each ensemble member\n",
    "scores = []\n",
    "\n",
    "# Same datetime-index\n",
    "Q_obs.index = Q_obs.index.normalize()\n",
    "Q_obs.index = Q_obs.index.tz_localize(None)\n",
    "\n",
    "# Slice only the overlap \n",
    "Q_obs_aligned = Q_obs.loc[df_ensemble.index.min():df_ensemble.index.max()]\n",
    "\n",
    "for i in range(n_particles):\n",
    "    sim_series = df_ensemble[f\"particle_{i}\"]\n",
    "\n",
    "    if sim_series is None or sim_series.empty:\n",
    "        print(f\"Skipping particle_{i} (empty output)\")\n",
    "        scores.append(np.inf)  # Assign a bad score\n",
    "        continue\n",
    "\n",
    "    # Use the correct observation series (make sure Q_obs exists and matches in index)\n",
    "    score = calibration_objective(sim_series, Q_obs_aligned)\n",
    "    scores.append(score)\n",
    "\n",
    "scores = np.array(scores)\n",
    "best_index = np.argmin(scores)              # index of minimum objective value\n",
    "best_score = scores[best_index]             # lowest score\n",
    "best_params = parameters[:, best_index]     # parameter set corresponding to best score\n",
    "\n",
    "print(f\"Best score: {best_score:.3f} (part {best_index})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8eb397-b432-4555-a5c7-ad9b286ec5f2",
   "metadata": {},
   "source": [
    "## Show the Optimal Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9477af8-b5ef-4fbe-8ea3-a1c33a10aef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best objective score: {best_score:.4f}\")\n",
    "\n",
    "# Round best parameters\n",
    "best_params_list = [round(val, 4) for val in best_params]\n",
    "\n",
    "# Define parameter descriptions (matches param_names)\n",
    "param_descriptions = [\n",
    "    \"Maximum intensity\",\n",
    "    \"Coefficient of evaporation\",\n",
    "    \"Field capacity\",\n",
    "    \"Shape coefficient\",\n",
    "    \"Maximum percolation rate\",\n",
    "    \"Time lag\",\n",
    "    \"Fast run-off parameter\",\n",
    "    \"Slow run-off parameter\",\n",
    "    \"Degree-day factor\"\n",
    "]\n",
    "\n",
    "# Build DataFrame\n",
    "df_params = pd.DataFrame({\n",
    "    \"Parameter\": param_names,\n",
    "    \"Description\": param_descriptions,\n",
    "    \"Value\": best_params_list\n",
    "})\n",
    "\n",
    "# Reset index and drop it for clean display\n",
    "df_params.index = [\"\"] * len(df_params)\n",
    "\n",
    "display(df_params.style.set_table_styles([\n",
    "    {\"selector\": \"th\", \"props\": [(\"font-weight\", \"bold\")]}\n",
    "]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
