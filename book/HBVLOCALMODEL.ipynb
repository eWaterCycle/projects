{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9cd8d85-eb4a-45d6-925a-e3e770592043",
   "metadata": {},
   "source": [
    "# Leven at Newby Bridge using HBV model in eWaterCycle\n",
    "This notebook demonstrates how eWaterCycle is used to run a model, in this case the classic HBV model, with forcing from the Caravan dataset.\n",
    "\n",
    "For this example, we chose a catchment in the Lake District in the UK. River discharge in river Leven is measured at the weirs at Newby Bridge, see the photo below. This observational data is available through the CamelsGB dataset [(Coxon, 2020)](https://essd.copernicus.org/articles/12/2459/2020/).\n",
    "The larger [Caravan](https://www.nature.com/articles/s41597-023-01975-w) (collection of Camels...) dataset is available through eWaterCycle and will be used below.\n",
    "\n",
    "![image](https://upload.wikimedia.org/wikipedia/commons/7/76/Weirs_on_the_River_Leven_at_Newby_Bridge_-_geograph.org.uk_-_5455774.jpg)\n",
    "\n",
    "*Weirs on the River Leven at Newby Bridge by G Laird*\n",
    "\n",
    "As a model we choose the classic [HBV model](https://hess.copernicus.org/articles/26/1371/2022/) for its simplicity. This nicely demonstrates how eWaterCycle works. More complex models are available through eWaterCycle, however:\n",
    "\n",
    "- these often require parameter sets specific to a region\n",
    "- these are more computationally intensive to run and therefore unsuited for a short 45 minute workshop.\n",
    "\n",
    "Ask us about available models if you want to collaborate! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641fbd17",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### HBV model structure\n",
    "\n",
    "The stucture of the HBV model is shown below.\n",
    "Do note that the figure is missing a snow reservoir, however this has been implemented in the HBV model in this notebook.\n",
    "\n",
    "![image](_images/model_layout.png)\n",
    "\n",
    "*Image from the TU Delft course ENVM1502 - \"River Basin Hydrology\" by Markus Hrachowitz* \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73ddde7-bde3-42e8-991a-8edd4825fd78",
   "metadata": {},
   "source": [
    "## Starting up\n",
    "To start up, we need to import eWaterCycle and a number of general Python libraries. 'Under the hood' eWaterCycle depends on a large number of other pieces of software, including but not limited to\n",
    "\n",
    "- [grpc4bmi](https://github.com/eWaterCycle/grpc4bmi), a 'translator' for BMI function calls between different programming languages and across containers. This library was build by the eWaterCycle team, but is available openly for anyone that can benefit from its functionality. \n",
    "- apptainer, a container engine that runs the model-containers (Docker is supported too).\n",
    "- [ESMValTool](https://github.com/ESMValGroup/ESMValTool), a climate data processing toolbox that originally intended to post-process climate data from CMIP projects for inclusion in IPCC reports, we adopted as tool for pre-processing climate data into forcing data for hydrological models\n",
    "- Numerous hydrological models that are made available as plugins to eWaterCycle, see [eWaterCycle-leakybucket](https://github.com/eWaterCycle/ewatercycle-leakybucket) as an example. Note that plugins do not have to be owned and maintained by the eWaterCycle team: anyone with a model can make a plugin for eWaterCycle and make their model be available for others through the platform. \n",
    "\n",
    "Furthermore, eWaterCycle requires forcing data, obsrvational data and parameter sets to be available to users. If you want to install eWaterCycle on your own infrastructure, see the [eWaterCycle documentation](https://ewatercycle.readthedocs.io/en/latest/system_setup.html) or contact us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec7e710a-5aa4-40f9-a1cb-151e3cddbe04",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-07T14:21:32.492461800Z",
     "start_time": "2024-03-07T14:21:31.890843300Z"
    }
   },
   "outputs": [],
   "source": [
    "# general python\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "\n",
    "#niceties\n",
    "from rich import print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4569a0f2-4bea-48cc-b5a4-ca5384e368c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-07T14:21:34.475126400Z",
     "start_time": "2024-03-07T14:21:32.537461400Z"
    }
   },
   "outputs": [],
   "source": [
    "# general eWaterCycle\n",
    "import ewatercycle\n",
    "import ewatercycle.models\n",
    "import ewatercycle.forcing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ecd862-3e33-4001-9218-d025f6acc2ae",
   "metadata": {},
   "source": [
    "## Choose region and time period: \n",
    "The HBV model is a lumped hydrological model. It considers a catchment as a homogenious area and calculates the major hydrological processes in it. It requires spatially aggregated rainfall and potential evaporation as inputs (ie. forcings). To calculate its modelled discharge at the outlet of the catchment it also needs a set of parameters. Usually these paramters are calibrated using (historical) observational data, so this is also required. \n",
    "\n",
    "in eWaterCycle we provide access to the Caravan dataset, which contains all of the above data for all the catchments in the different Camels datasets. In this notebook we use the precipitation and evaporation data from Caravan. However, there is a known problem with the caravan evaporation data and (the current version) shouldn't be used. In this notebook we do so to demonstrate how it works. It would be better to generate forcing from (for example) ERA5. [This](example_model_run_HBV_camels_catchment_ERA5_forcing.ipynb) additional notebook that looks very similar to this one explains how to do that. \n",
    "\n",
    "Using the interactive maps at [eWaterCycle caravan map](https://www.ewatercycle.org/caravan-map/) one can easily retrieve the identifier of the catchment.\n",
    "\n",
    "Note that changing the region below will work, but that the parameters that are loaded later in this notebook are calibrated specifically for this particular catchment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5898939-54a6-40de-8a27-bbc793536248",
   "metadata": {},
   "outputs": [],
   "source": [
    "camelsgb_id = \"lamah_208082\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b5fc86-ec75-46a8-9f40-4685444628e5",
   "metadata": {},
   "source": [
    "We have to specify start and end date of the experiment that we want to do. For now we don't fuzz with diverences between calibration and validation periods (which officially of course is very bad...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03aea008-87ce-4d09-8d01-f12dfe6bb116",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-07T14:21:34.567126600Z",
     "start_time": "2024-03-07T14:21:34.567126600Z"
    }
   },
   "outputs": [],
   "source": [
    "experiment_start_date = \"1979-08-01T00:00:00Z\"\n",
    "experiment_end_date = \"2023-08-31T00:00:00Z\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111bc65b-8299-43ba-95fd-e92df6b92707",
   "metadata": {},
   "source": [
    "## Set up paths\n",
    "\n",
    "Since forcing files are often re-used between experiments it is a best practice to save those intermediate files for re-use between experiments. These logical save-points in workflows are called 'rustpunten' in Dutch. It is important to store data in 'rustpunten' in standard formats. Working with clearly defined 'rustpunten' is a key element in the design of good workflows in general and was instrumental in designing eWaterCycle in particular. \n",
    "\n",
    "Here we set up some paths to store the forcing files we generate in your own home directory. \n",
    "\n",
    "To speed up this workshop, we have already created the forcing files in a central location, which we also create pointers to here. If you want to run for a different region, you will have to generate the forcing yourself. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb5ed837-14aa-4629-b589-ffc33c24ba84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Even though we don't use caravan data as forcing, we still call it forcing\n",
    "# because it is generated using the forcing module of eWaterCycle\n",
    "forcing_path_caravan = Path.home() / \"forcing\" / camelsgb_id / \"caravan\"\n",
    "forcing_path_caravan.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# If someone has prepared forcing for you, this path needs to be changed to that location. \n",
    "prepared_forcing_path_caravan_central = Path(\"location/of/forcing/data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1171ca7e-d26a-4cbe-abb0-d741315a708c",
   "metadata": {},
   "source": [
    "## Generate or load forcing\n",
    "There are three options for creating forcing data objects:\n",
    "\n",
    "- generate from climate data such as Caravan, ERA5 or CMIP. Note that if the directory you specify as destination already contains data this trying this will throw an error!\n",
    "- load forcing data you generated previously by providing the location where it was stored\n",
    "- load forcing data someone else (such as your teacher or a workshop leader) generated previously by providing the location where it was stored\n",
    "\n",
    "First we will create a caravan forcing object, but as mentioned above, we will only use this for the discharge observations and the shape file of the region. After generating the object we show the fields it contains and plot the discharge data. \n",
    "\n",
    "The actual forcing object we will use in this example is the ERA5 based data.\n",
    "\n",
    "For both caravan and ERA5 based forcing, only one of the options provided below should be used, use comments to select which one you want to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19641ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "syntax error, unexpected WORD_WORD, expecting SCAN_ATTR or SCAN_DATASET or SCAN_ERROR\n",
      "context: <!DOCTYPE^ html PUBLIC \"-//W3C//DTD XHTML 1.1//EN\" \"http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd\"><html xmlns=\"http://www.w3.org/1999/xhtml\" xml:lang=\"en\"> <head> <title>The page is temporarily unavailable</title> <meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\" /> <style type=\"text/css\"> /*<![CDATA[*/ body { background-color: #fff; color: #000; font-size: 0.9em; font-family: sans-serif,helvetica; margin: 0; padding: 0; } :link { color: #c00; } :visited { color: #c00; } a:hover { color: #f50; } h1 { text-align: center; margin: 0; padding: 0.6em 2em 0.4em; background-color: #900; color: #fff; font-weight: normal; font-size: 1.75em; border-bottom: 2px solid #000; } h1 strong { font-weight: bold; font-size: 1.5em; } h2 { text-align: center; background-color: #900; font-size: 1.1em; font-weight: bold; color: #fff; margin: 0; padding: 0.5em; border-bottom: 2px solid #000; } h3 { text-align: center; background-color: #ff0000; padding: 0.5em; color: #fff; } hr { display: none; } .content { padding: 1em 5em; } .alert { border: 2px solid #000; } img { border: 2px solid #fff; padding: 2px; margin: 2px; } a:hover img { border: 2px solid #294172; } .logos { margin: 1em; text-align: center; } /*]]>*/ </style> </head> <body> <h1><strong>nginx error!</strong></h1> <div class=\"content\"> <h3>The page you are looking for is temporarily unavailable. Please try again later.</h3> <div class=\"alert\"> <h2>Website Administrator</h2> <div class=\"content\"> <p>Something has triggered missing webpage on your website. This is the default error page for <strong>nginx</strong> that is distributed with Red Hat Enterprise Linux. It is located <tt>/usr/share/nginx/html/50x.html</tt></p> <p>You should customize this error page for your own site or edit the <tt>error_page</tt> directive in the <strong>nginx</strong> configuration file <tt>/etc/nginx/nginx.conf</tt>.</p> <p>For information on Red Hat Enterprise Linux, please visit the <a href=\"http://www.redhat.com/\">Red Hat, Inc. website</a>. The documentation for Red Hat Enterprise Linux is <a href=\"http://www.redhat.com/docs/manuals/enterprise/\">available on the Red Hat, Inc. website</a>.</p> </div> </div> <div class=\"logos\"> <a href=\"http://nginx.net/\"><img src=\"nginx-logo.png\"  alt=\"[ Powered by nginx ]\" width=\"121\" height=\"32\" /></a> <a href=\"http://www.redhat.com/\"><img src=\"poweredby.png\" alt=\"[ Powered by Red Hat Enterprise Linux ]\" width=\"88\" height=\"31\" /></a> </div> </div> </body></html>\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno -70] NetCDF: DAP server error: 'https://opendap.4tu.nl/thredds/dodsC/data2/djht/ca13056c-c347-4a27-b320-930c2a4dd207/1/lamah.nc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/envs/ewatercycle2/lib/python3.12/site-packages/xarray/backends/file_manager.py:211\u001b[0m, in \u001b[0;36mCachingFileManager._acquire_with_cache_info\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 211\u001b[0m     file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_key\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/envs/ewatercycle2/lib/python3.12/site-packages/xarray/backends/lru_cache.py:56\u001b[0m, in \u001b[0;36mLRUCache.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m---> 56\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache\u001b[38;5;241m.\u001b[39mmove_to_end(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: [<class 'netCDF4._netCDF4.Dataset'>, ('https://opendap.4tu.nl/thredds/dodsC/data2/djht/ca13056c-c347-4a27-b320-930c2a4dd207/1/lamah.nc',), 'r', (('clobber', True), ('diskless', False), ('format', 'NETCDF4'), ('persist', False)), '0180706c-44bb-459e-8e35-fa5b56b501e4']",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# option one: generate forcing data\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m camelsgb_forcing \u001b[38;5;241m=\u001b[39m \u001b[43mewatercycle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforcing\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msources\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCaravanForcing\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstart_time\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexperiment_start_date\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mend_time\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexperiment_end_date\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdirectory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforcing_path_caravan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbasin_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcamelsgb_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/ewatercycle2/lib/python3.12/site-packages/ewatercycle/_forcings/caravan.py:187\u001b[0m, in \u001b[0;36mCaravanForcing.generate\u001b[0;34m(cls, start_time, end_time, directory, variables, shape, **kwargs)\u001b[0m\n\u001b[1;32m    184\u001b[0m basin_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbasin_id\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    186\u001b[0m dataset: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m basin_id\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 187\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    188\u001b[0m ds_basin \u001b[38;5;241m=\u001b[39m ds\u001b[38;5;241m.\u001b[39msel(basin_id\u001b[38;5;241m=\u001b[39mbasin_id\u001b[38;5;241m.\u001b[39mencode())\n\u001b[1;32m    189\u001b[0m ds_basin_time \u001b[38;5;241m=\u001b[39m crop_ds(ds_basin, start_time, end_time)\n",
      "File \u001b[0;32m/opt/conda/envs/ewatercycle2/lib/python3.12/site-packages/ewatercycle/_forcings/caravan.py:121\u001b[0m, in \u001b[0;36mCaravanForcing.get_dataset\u001b[0;34m(cls, dataset)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_dataset\u001b[39m(\u001b[38;5;28mcls\u001b[39m: \u001b[38;5;28mtype\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCaravanForcing\u001b[39m\u001b[38;5;124m\"\u001b[39m], dataset: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m xr\u001b[38;5;241m.\u001b[39mDataset:\n\u001b[1;32m    109\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Opens specified dataset from data.4tu.nl OPeNDAP server.\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \n\u001b[1;32m    111\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;124;03m            'lamah'\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mxr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mOPENDAP_URL\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdataset\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.nc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/ewatercycle2/lib/python3.12/site-packages/xarray/backends/api.py:670\u001b[0m, in \u001b[0;36mopen_dataset\u001b[0;34m(filename_or_obj, engine, chunks, cache, decode_cf, mask_and_scale, decode_times, decode_timedelta, use_cftime, concat_characters, decode_coords, drop_variables, inline_array, chunked_array_type, from_array_kwargs, backend_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    658\u001b[0m decoders \u001b[38;5;241m=\u001b[39m _resolve_decoders_kwargs(\n\u001b[1;32m    659\u001b[0m     decode_cf,\n\u001b[1;32m    660\u001b[0m     open_backend_dataset_parameters\u001b[38;5;241m=\u001b[39mbackend\u001b[38;5;241m.\u001b[39mopen_dataset_parameters,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    666\u001b[0m     decode_coords\u001b[38;5;241m=\u001b[39mdecode_coords,\n\u001b[1;32m    667\u001b[0m )\n\u001b[1;32m    669\u001b[0m overwrite_encoded_chunks \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite_encoded_chunks\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 670\u001b[0m backend_ds \u001b[38;5;241m=\u001b[39m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdrop_variables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_variables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdecoders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    676\u001b[0m ds \u001b[38;5;241m=\u001b[39m _dataset_from_backend_dataset(\n\u001b[1;32m    677\u001b[0m     backend_ds,\n\u001b[1;32m    678\u001b[0m     filename_or_obj,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    688\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    689\u001b[0m )\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ds\n",
      "File \u001b[0;32m/opt/conda/envs/ewatercycle2/lib/python3.12/site-packages/xarray/backends/netCDF4_.py:666\u001b[0m, in \u001b[0;36mNetCDF4BackendEntrypoint.open_dataset\u001b[0;34m(self, filename_or_obj, mask_and_scale, decode_times, concat_characters, decode_coords, drop_variables, use_cftime, decode_timedelta, group, mode, format, clobber, diskless, persist, auto_complex, lock, autoclose)\u001b[0m\n\u001b[1;32m    644\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mopen_dataset\u001b[39m(\n\u001b[1;32m    645\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    646\u001b[0m     filename_or_obj: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m os\u001b[38;5;241m.\u001b[39mPathLike[Any] \u001b[38;5;241m|\u001b[39m ReadBuffer \u001b[38;5;241m|\u001b[39m AbstractDataStore,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    663\u001b[0m     autoclose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    664\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dataset:\n\u001b[1;32m    665\u001b[0m     filename_or_obj \u001b[38;5;241m=\u001b[39m _normalize_path(filename_or_obj)\n\u001b[0;32m--> 666\u001b[0m     store \u001b[38;5;241m=\u001b[39m \u001b[43mNetCDF4DataStore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    667\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclobber\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclobber\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdiskless\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdiskless\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpersist\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpersist\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mauto_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauto_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautoclose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautoclose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    679\u001b[0m     store_entrypoint \u001b[38;5;241m=\u001b[39m StoreBackendEntrypoint()\n\u001b[1;32m    680\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m close_on_error(store):\n",
      "File \u001b[0;32m/opt/conda/envs/ewatercycle2/lib/python3.12/site-packages/xarray/backends/netCDF4_.py:452\u001b[0m, in \u001b[0;36mNetCDF4DataStore.open\u001b[0;34m(cls, filename, mode, format, group, clobber, diskless, persist, auto_complex, lock, lock_maker, autoclose)\u001b[0m\n\u001b[1;32m    448\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_complex\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m auto_complex\n\u001b[1;32m    449\u001b[0m manager \u001b[38;5;241m=\u001b[39m CachingFileManager(\n\u001b[1;32m    450\u001b[0m     netCDF4\u001b[38;5;241m.\u001b[39mDataset, filename, mode\u001b[38;5;241m=\u001b[39mmode, kwargs\u001b[38;5;241m=\u001b[39mkwargs\n\u001b[1;32m    451\u001b[0m )\n\u001b[0;32m--> 452\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmanager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mautoclose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautoclose\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/ewatercycle2/lib/python3.12/site-packages/xarray/backends/netCDF4_.py:393\u001b[0m, in \u001b[0;36mNetCDF4DataStore.__init__\u001b[0;34m(self, manager, group, mode, lock, autoclose)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_group \u001b[38;5;241m=\u001b[39m group\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;241m=\u001b[39m mode\n\u001b[0;32m--> 393\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mds\u001b[49m\u001b[38;5;241m.\u001b[39mdata_model\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mds\u001b[38;5;241m.\u001b[39mfilepath()\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_remote \u001b[38;5;241m=\u001b[39m is_remote_uri(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_filename)\n",
      "File \u001b[0;32m/opt/conda/envs/ewatercycle2/lib/python3.12/site-packages/xarray/backends/netCDF4_.py:461\u001b[0m, in \u001b[0;36mNetCDF4DataStore.ds\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mds\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 461\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_acquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/ewatercycle2/lib/python3.12/site-packages/xarray/backends/netCDF4_.py:455\u001b[0m, in \u001b[0;36mNetCDF4DataStore._acquire\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_acquire\u001b[39m(\u001b[38;5;28mself\u001b[39m, needs_lock\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m--> 455\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_manager\u001b[38;5;241m.\u001b[39macquire_context(needs_lock) \u001b[38;5;28;01mas\u001b[39;00m root:\n\u001b[1;32m    456\u001b[0m         ds \u001b[38;5;241m=\u001b[39m _nc4_require_group(root, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_group, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode)\n\u001b[1;32m    457\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ds\n",
      "File \u001b[0;32m/opt/conda/envs/ewatercycle2/lib/python3.12/contextlib.py:137\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt yield\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/ewatercycle2/lib/python3.12/site-packages/xarray/backends/file_manager.py:199\u001b[0m, in \u001b[0;36mCachingFileManager.acquire_context\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;129m@contextlib\u001b[39m\u001b[38;5;241m.\u001b[39mcontextmanager\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21macquire_context\u001b[39m(\u001b[38;5;28mself\u001b[39m, needs_lock\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    198\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Context manager for acquiring a file.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 199\u001b[0m     file, cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_acquire_with_cache_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mneeds_lock\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    201\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m file\n",
      "File \u001b[0;32m/opt/conda/envs/ewatercycle2/lib/python3.12/site-packages/xarray/backends/file_manager.py:217\u001b[0m, in \u001b[0;36mCachingFileManager._acquire_with_cache_info\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    215\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    216\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode\n\u001b[0;32m--> 217\u001b[0m file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_opener\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;66;03m# ensure file doesn't get overridden when opened again\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32msrc/netCDF4/_netCDF4.pyx:2470\u001b[0m, in \u001b[0;36mnetCDF4._netCDF4.Dataset.__init__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/netCDF4/_netCDF4.pyx:2107\u001b[0m, in \u001b[0;36mnetCDF4._netCDF4._ensure_nc_success\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno -70] NetCDF: DAP server error: 'https://opendap.4tu.nl/thredds/dodsC/data2/djht/ca13056c-c347-4a27-b320-930c2a4dd207/1/lamah.nc'"
     ]
    }
   ],
   "source": [
    "# option one: generate forcing data\n",
    "camelsgb_forcing = ewatercycle.forcing.sources['CaravanForcing'].generate(\n",
    "    start_time=experiment_start_date,\n",
    "    end_time=experiment_end_date,\n",
    "    directory=forcing_path_caravan,\n",
    "    basin_id=camelsgb_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35875c4f-d5f0-465c-b349-82580dbd71bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # option two or three: load data that you or someone else generated previously\n",
    "# camelsgb_forcing = ewatercycle.forcing.sources['CaravanForcing'].load(directory=prepared_forcing_path_caravan_central)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b696cee-5dbe-40de-bafb-6346f6265ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(camelsgb_forcing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc974bcb-5489-4af6-8cf9-cc0299adab97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#quick plot of the discharge data. \n",
    "ds_forcing = xr.open_mfdataset([camelsgb_forcing['Q'],camelsgb_forcing['pr'],camelsgb_forcing['evspsblpot']])\n",
    "ds_forcing[\"Q\"].plot()\n",
    "\n",
    "#Load the forcing dataset\n",
    "ds = xr.open_dataset(\"/home/thirza/forcing/lamah_208082/caravan/lamah_208082_1930-08-01_2024-08-31_pr.nc\")\n",
    "\n",
    "catchment_area = ds[\"area\"].values\n",
    "\n",
    "# go from mm/day to m3/s\n",
    "discharge = []\n",
    "for i in range(len(ds_forcing[\"Q\"].values)):\n",
    "    discharge.append((ds_forcing[\"Q\"].values[i] * catchment_area * 1000) / (24 * 60 * 60))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e6de26-dc30-438e-8fc2-44161b9d3d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ds[\"time\"].values\n",
    "y = discharge\n",
    "\n",
    "plt.plot(x,y)\n",
    "plt.xlabel('time')\n",
    "plt.ylabel('observed streaflow [$m^3$/s]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f55cfac-eef3-4ede-b7f7-7ea25a646b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "max(discharge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362996b1-6b33-4938-96f6-70e9cc3e59d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# waardes en aantal van deze waardes boven een drempel bepalen\n",
    "print([x for x in discharge if x >= 20])\n",
    "print(len([x for x in discharge if x >= 20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173e31f7-23da-4c15-8bf8-a1eb4bd77256",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6509bef-bb85-45c3-a6e4-8d092e387d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#see how the Q values are distributed\n",
    "plt.hist(discharge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab9aced-1bd1-47fa-bbfe-f032ffb8afb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#see how high Q values are distributed\n",
    "plt.hist([value for value in discharge if value > 20], bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a6def1-ad5d-4c8f-bf0f-3514fa0fd1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the maximum annual discharge\n",
    "max_discharge = ds_forcing[\"Q\"].groupby(\"time.year\").max()\n",
    "maxdischarge = max_discharge.values * catchment_area * 1000 / (24 * 60 * 60) #convert to m3/s\n",
    "# print(maxdischarge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cdae26-dd2d-4aaa-abb9-8ced25d01ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "import math\n",
    "# # discharge data > 20 m³/s\n",
    "# data = [value for value in discharge if value > 20]\n",
    "\n",
    "# all discharge data\n",
    "cleaned_discharge = [x for x in discharge if not math.isnan(x)]\n",
    "data = cleaned_discharge\n",
    "\n",
    "# maximum annual discharge\n",
    "# data = [x for x in maxdischarge if not math.isnan(x)]\n",
    "# # print(data)\n",
    "\n",
    "# Generalized Extreme Value (GEV) distribution \n",
    "shape, loc, scale = stats.genextreme.fit(data)\n",
    "\n",
    "# Define threshold value\n",
    "threshold = 530\n",
    "\n",
    "# calculate exceedance probability in years\n",
    "p = 1 - stats.genextreme.cdf(threshold, shape, loc=loc, scale=scale)\n",
    "\n",
    "# calculate return period in years\n",
    "T = 1 / p / 365.25\n",
    "# print(T)\n",
    "\n",
    "# calculate change threshold exceedance at least once in 1000 and 1100 years\n",
    "P_1000 = 1 - (1 - p) ** 1000\n",
    "P_1100 = 1 - (1 - p) ** 1100\n",
    "\n",
    "# Results 1000\n",
    "print(f\"Returnperiod for 530 m³/s: {T:.2f} years\")\n",
    "print(f\"Chance of at least one exceedance in 1000 years: {P_1000:.5f} ({P_1000 * 100:.2f}%)\")\n",
    "print(f\"Chance of at least one exceedance in 1100 years: {P_1100:.5f} ({P_1100 * 100:.2f}%)\")\n",
    "\n",
    "# expected amount of times threshold is exceeded in 1000 years\n",
    "expected_exceedances1000 = 1000 / T if T > 0 else 0\n",
    "print(f\"The discharge currently exceeds the threshold of 530 m³/s {expected_exceedances1000:.3f} times in 1000 years\")\n",
    "\n",
    "# expected amount threshold is exceeded in 1100 years\n",
    "expected_exceedances1100 = 1100 / T if T > 0 else 0\n",
    "print(f\"The discharge currently exceeds the threshold of 530 m³/s {expected_exceedances1100:.3f} times in 1100 years\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27a4599-de8c-4d72-bf70-d63bec1b2e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "import math\n",
    "# # discharge data > 20 m³/s\n",
    "# data = [value for value in discharge if value > 20]\n",
    "\n",
    "# all discharge data\n",
    "cleaned_discharge = [x for x in discharge if not math.isnan(x)]\n",
    "data = cleaned_discharge\n",
    "\n",
    "# maximum annual discharge\n",
    "# data = [x for x in maxdischarge if not math.isnan(x)]\n",
    "# # print(data)\n",
    "\n",
    "# Generalized Extreme Value (GEV) distribution \n",
    "shape, loc, scale = stats.genextreme.fit(data)\n",
    "\n",
    "# Define threshold value\n",
    "threshold = np.linspace(1, 600, 600)\n",
    "\n",
    "return_period = []\n",
    "prob = []\n",
    "for i in range(len(threshold)):\n",
    "# calculate exceedance probability in years\n",
    "    p = 1 - stats.genextreme.cdf(threshold[i], shape, loc=loc, scale=scale)\n",
    "    prob.append(p)\n",
    "# calculate return period in years\n",
    "    T = 1 / p / 365.25\n",
    "    return_period.append(T)\n",
    "# print(return_period)\n",
    "\n",
    "plt.plot(return_period, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d14bd1-ec6b-49fb-a70f-1b64b0688029",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_forcing[\"pr\"].plot(label = 'precipitation')\n",
    "ds_forcing[\"evspsblpot\"].plot(label = 'potential evaporation')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d9f97f-c40b-4ea0-a8e6-1a54840df55e",
   "metadata": {},
   "source": [
    "Below we generate forcing from ERA5. Note that ERA5 data needs to be stored on the system you are working on. For SURF Research Cloud machines running the eWaterCycle catalog item, this is handled by the eWaterCycle team. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4787c692-3f9c-402b-9b48-93daeeb47926",
   "metadata": {},
   "source": [
    "## Load parameters from calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4557dc0-b086-4230-8368-ef9b91cb2711",
   "metadata": {},
   "source": [
    "The HBV model contains five \"stores\" where the water is stored and nine parameters that control the flow between those stores and in and out of the model.\n",
    "We have already calibrated the model for our region of choice and hardcoded those below. If you have changed the region and have calibrated for your region, you need to point to your calibration results here by uncommenting the load statement and pointing to the right file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f7d956-13c4-4954-a563-94add8f425af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-calibrated constants. If you have calibrated for a different region, comment this part and \n",
    "# uncomment the load statement below\n",
    "#par_0 = [7.085, 0.837, 76.373, 1.112, 0.245, 7.801, 0.096, 0.003, 0.226];\n",
    "\n",
    "par_0 = [ 35.66528028,   1.1858021,  170.34055787,   1.46199007,   1.91737973,\n",
    "   1.45432159,   0.57091109,   1.88674579,   0.83621774]\n",
    "\n",
    "\n",
    "# #load calibration constants from a csv file\n",
    "# par_0 = np.loadtxt(\"your/calibration/file.csv\", delimiter = \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed960ab6-b63a-4664-8306-b57c2ecf27ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-07T14:21:34.528887900Z",
     "start_time": "2024-03-07T14:21:34.501125300Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "param_names = [\"Imax\", \"Ce\", \"Sumax\", \"Beta\", \"Pmax\", \"Tlag\", \"Kf\", \"Ks\", \"FM\"]\n",
    "print(list(zip(param_names, np.round(par_0, decimals=3))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324ab8b0-947b-4116-84d9-e476d46ddd98",
   "metadata": {},
   "source": [
    "For the storages we can specify an array of starting values. If you don't the model will start 'empty' and needs some timesteps to 'fill up'. Especially for the rootzone storage it helps to not start empty. Note that all units are in mm: <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70032e1-602a-4163-a338-2a3ff6264c00",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-07T14:21:34.494347400Z",
     "start_time": "2024-03-07T14:21:34.484123900Z"
    }
   },
   "outputs": [],
   "source": [
    "#               Si,  Su, Sf, Ss, Sp\n",
    "s_0 = np.array([0,  100,  0,  5,  0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff45d384-3549-414b-b8f2-793399161aaf",
   "metadata": {},
   "source": [
    "## Setting up the model\n",
    "Below we show the core use of eWaterCycle: to users models are objects. We stay as close as possible to the standard BMI functions, but make sure that under the hood eWaterCycle makes everything run. Getting a model to run requires three steps:\n",
    "\n",
    "1. creating a model object, an instance of the specific model class. This is provided by the different plugins. At the point of creation, the forcing object that will be used need to be passed to the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62a1be2-e5a1-4c82-aaf0-9ab2eeea14af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-07T14:21:34.569218800Z",
     "start_time": "2024-03-07T14:21:34.568127600Z"
    }
   },
   "outputs": [],
   "source": [
    "model = ewatercycle.models.HBVLocal(forcing=camelsgb_forcing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c0bd34",
   "metadata": {},
   "source": [
    "2. creating a configuration file that contains all the information the model needs to initialize itself.\n",
    "   The format of the configuration file is very model specific. For example, the HBV configuration file contains information on:\n",
    "  - the location of forcing files\n",
    "  - the values of parameters and initial storages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbd898f-b3cc-4d03-8050-21067d31c1e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-07T14:21:34.948731400Z",
     "start_time": "2024-03-07T14:21:34.568127600Z"
    }
   },
   "outputs": [],
   "source": [
    "config_file, _ = model.setup(parameters=par_0, initial_storage=s_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbb274a",
   "metadata": {},
   "source": [
    "3. initializing starts the model container, creates all variables, and basically gets the model primed to start running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba14b408-e446-45aa-b406-91dc35f40bc1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-07T14:21:34.957732500Z",
     "start_time": "2024-03-07T14:21:34.953733500Z"
    }
   },
   "outputs": [],
   "source": [
    "model.initialize(config_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e320191-4ee4-4535-94f3-05e5afef96e2",
   "metadata": {},
   "source": [
    "## Running the model\n",
    "The basic loop that runs the model calls the ```model.update()``` to have the model progress one timestep and ```model.get_value()``` to extract information of interest. More complex experiment can interact with the model using, for example, ```model.set_value()``` to change values. In this way \n",
    "\n",
    "- multiple models can interact (including be coupled)\n",
    "- models can be adjusted during runtime to incoming observations (ie. data assimilation)\n",
    "- Effects not covered by the model can be calculated seperatly and included to create 'what if' experiments.\n",
    "- and many more applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579fe3d8-0418-4e76-9d66-9c1a938812cc",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-07T14:21:34.997729900Z"
    }
   },
   "outputs": [],
   "source": [
    "Q_m = []\n",
    "time = []\n",
    "while model.time < model.end_time:\n",
    "    model.update()\n",
    "    Q_m.append(model.get_value(\"Q\")[0])\n",
    "    time.append(pd.Timestamp(model.time_as_datetime))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a78bda-4e19-474f-8367-acec10c50f52",
   "metadata": {},
   "source": [
    "After running the model we need to call ```model.finalize()``` to shut everything down, including the container. If we don't do this, the container will continue to be active, eating up system memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fd6178-1059-497a-a91f-dd0fa983df2a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-07T14:21:35.030728200Z",
     "start_time": "2024-03-07T14:21:34.997729900Z"
    }
   },
   "outputs": [],
   "source": [
    "model.finalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b88262e-c750-4f8e-a602-50c404969690",
   "metadata": {},
   "source": [
    "## Process results\n",
    "Finally, we use standard python libraries to visualize the results. We put the model output into a pandas Series to make plotting easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccde9dca-258d-409c-a602-d4abda553b32",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-07T14:21:34.997729900Z"
    }
   },
   "outputs": [],
   "source": [
    "model_output = pd.Series(data=Q_m, name=\"Modelled_discharge\", index=time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02879a8d-c55f-4a74-a6cf-8154cffb3d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ds_forcing[\"Q\"].plot(label=\"Observed discharge\")\n",
    "model_output.plot()\n",
    "plt.legend()\n",
    "plt.ylabel(\"Discharge (mm/d)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee33066a-f0a0-41fb-a3fb-b07fd5f3ced5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b543aa67-dde1-4c50-b6ac-657c75eecf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we want to also be able to use the output of this model run in different analyses. Therefore we save it as a NetCDF file\n",
    "xr_model_output = model_output.to_xarray()\n",
    "\n",
    "xr_model_output.attrs['units'] = 'mm/d'\n",
    "\n",
    "# Save the xarray Dataset to a NetCDF file\n",
    "xr_model_output.to_netcdf('~/river_discharge_data.nc')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
